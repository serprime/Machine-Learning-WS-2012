%x%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Short Sectioned Assignment
% LaTeX Template
% Version 1.0 (5/5/12)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Frits Wenneker (http://www.howtotex.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%   PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[paper=a4, fontsize=11pt]{scrartcl} % A4 paper and 11pt font size

\usepackage{color}
\usepackage{float}
\usepackage{placeins}
\usepackage{natbib}
% philipp: added for code listings
\usepackage{listings}
\lstset{
    captionpos=b,
    basicstyle=\small
}
\newcommand{\Hilight}{\makebox[0pt][l]{\color{red}\rule[-4pt]{0.65\linewidth}{14pt}}}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{morefloats}
\bibliographystyle{plain}

\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
\usepackage{fourier} % Use the Adobe Utopia font for the document - comment this line to return to the LaTeX default
\usepackage[english]{babel} % English language/hyphenation
\usepackage{amsmath,amsfonts,amsthm} % Math packages

\usepackage{lipsum} % Used for inserting dummy 'Lorem ipsum' text into the template
\usepackage{todonotes}

\usepackage{sectsty} % Allows customizing section commands
\allsectionsfont{\centering \normalfont\scshape} % Make all sections centered, the default font and small caps

\usepackage{fancyhdr} % Custom headers and footers
\pagestyle{fancyplain} % Makes all pages in the document conform to the custom headers and footers
\fancyhead{} % No page header - if you want one, create it in the same way as the footers below
\fancyfoot[L]{} % Empty left footer
\fancyfoot[C]{} % Empty center footer
\fancyfoot[R]{\thepage} % Page numbering for right footer
% \renewcommand{\thesection}{Data Set \arabic{section}}
\renewcommand{\headrulewidth}{0pt} % Remove header underlines
\renewcommand{\footrulewidth}{0pt} % Remove footer underlines
\setlength{\headheight}{13.6pt} % Customize the height of the header

\numberwithin{equation}{section} % Number equations within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{figure}{section} % Number figures within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{table}{section} % Number tables within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)

\setlength{\parskip}{\baselineskip}%
\setlength\parindent{0pt} % Removes all indentation from paragraphs - comment this line for an assignment with lots of text
%----------------------------------------------------------------------------------------
%   TITLE SECTION
%----------------------------------------------------------------------------------------

\newcommand{\horrule}[1]{\rule{\linewidth}{#1}} % Create horizontal rule command with 1 argument of height

\title{ 
\normalfont \normalsize 
\textsc{Vienna University of Technology} \\ [25pt] % Your university, school and/or department name(s)
\horrule{0.5pt} \\[0.4cm] % Thin top horizontal rule
\huge Experiments in Machine Learning 2 \\ % The assignment title
\horrule{2pt} \\[0.5cm] % Thick bottom horizontal rule
}

\author{Benjamin Kiesl \and Philipp Steinwender \and Robert Sch\"{a}fer} % Your name

\date{\normalsize\today} % Today's date or a custom date

\begin{document}

\maketitle % Print the title

%----------------------------------------------------------------------------------------
%   PROBLEM 1
%----------------------------------------------------------------------------------------

\tableofcontents

%
%
%
\section{TODO}

\begin{itemize}
\item Implementation of cutsom bn-searcher
\item documentation of cutsom bn-searcher
\item comparison and interpretation of the results
\end{itemize}

%
%
%
\section{Preface}

This assignment is about extending a Bayes Network Classifier by improving an existing search heuristic in Weka. We chose the Hill Climber search algorithm for an implementation of an iterative local search algorithm.

%
%
%
\section{Setup}

There is a R script to execute the searcher in the handed in package at \texttt{R/bayesNetTest.R}. It loads Weka, builds the classifier, runs it and evaluates the model. It also shows the performance tables and the differences of the original Hill Climber implementation and our improved one. To run the script the following Tools need to be installed: R with the packages RWeka and rJava, Java (JRE).

The used package RWeka is an interface in R that wraps the Weka framework and has a really good documentation.

%
%
%\section{Weka-Implementation of the Bayesian Network}
%
%
%First we take a look at the BayesNet classifier implementation. When the classifier is startet, first thing is to generate the classification model. Therefore the BayesNet classifier initializes an adjacency matrix (\texttt{initStructure()}) where each attribute has an array of parents. These arrays are empty after initialization. 
%
%%
%%\begin{lstlisting}[caption={Initialization of BayesNet},label=initStructure]
%%...
%%// initialize parent sets to have arrow from classifier node to
%%// each of the other nodes
%%for (int iAttribute = 0; iAttribute < instances.numAttributes(); iAttribute++) {
%%    if (iAttribute != iClass) {
%%        bayesNet.getParentSet(iAttribute).addParent(iClass, instances);
%%    }
%%}
%%...
%%\end{lstlisting}
%
%The default Weka's SearchAlgorithm initializes the network like a Naive Bayes network. Listing \ref{initStructure} shows how we can use the adjacency matrix of BayesNet. Where \texttt{BayesNet} is the classifier that holds the network datastructure and \texttt{getParentSet(n)} returns the \texttt{nth} row in the adjacency matrix.

%
%
%
\section{Weka-Implementation of Hill Climber}

We chose to implement an iterated local search which is based on the pre-implemented Hill Climber algorithm in Weka. There exist implementations as local score based algorithm and as global score based algorithm.  We chose the global score based algorithm because this is better suited to iterated local search (for the perturbation and comparison with other solutions). The existing Hill Climber algorithm does the following:

\begin{enumerate}
	\item Create a start solution: One can choose between an empty network which contains no arcs or a Naive Bayes network as start solution. In the Naive Bayes network the classification node is the parent of all other attribute nodes.
	\item Local search: The algorithm chooses always the best solution from its neighborhood. The neighborhood is defined as all solutions which are accessible via the following operations:
		\begin{itemize}
			\item Add a new arc.
			\item Delete an existing arc.
			\item Reverse an existing arc.
		\end{itemize}
		As normal in local search the neighborhood is exploited until a local optimum is reached. The best solution is then returned as the Bayesian network.
\end{enumerate}

\section{Classification results of the pre-implemented Hill Climber}

\todo{give a table with the results for hill climber with empty start solution as well as with naive bayes.}

\section{Improvement of the Hill Climber: Iterated Local Search}

We improved the existing Hill Climber algorithm by making an iterated local search out of it. The basic functionality is as follows:

\begin{itemize}
	\item Create a start solution.
	\item Apply local search to the start solution. The solution obtained is the new best solution.
	\item Repeat the following procedure until there is no new best solution for five iterations:
		\begin{itemize}
			\item Perturbate the current best solution.
			\item Apply local search to the perturbated solution.
			\item If the solution obtained after local search is better than the current best solution then this solution is the new best solution.
		\end{itemize}
\end{itemize}

It remains to describe what the Perturbation-procedure does: The perturbation-procedure modifies the given net by applying some number of random moves in the neighborhood. We implemented the search procedure in a way such that it is possible for the user to choose the number of random moves which are applied in every perturbation step. The standard value of random moves is three. Since it is not always possible to apply a move there are some tests to make sure that a random move is possible. This is necessary in the following cases:

\begin{itemize}
	\item The network doesn't contain an arc. (Deletion of an arc is not allowed/possible.)
	\item Every node has a maximum number of parents. (Addition of an arc is not allowed/possible.)
	\item Addition of an arc creates a cycle. (This test is pre-implemented in Weka.
	\item etc.
\end{itemize}

\section{Classification results of the iterated local search}

\todo{Insert tables containing the classification results of the iterated local search}.

\section{Comparison of the Hill Climber algorithm and the iterated local search}

One can see that the iterated local search does not or in some cases just slightly perform better than the pre-implemented Hill-Climber algorithm. This is mostly due to the fact that for our test data sets the Naive Bayes representation yields the best or at least nearly the best result. The Hill climber doesn't perform many neighborhood operations. For the contraception data set there are no neighborhood operations at all and for the adult data set its just a few operations (about five operations). The perturbation normally leads to worse solutions and local search then finds a net which is close to Naive Bayes or pretty similar to it so we don't get a better solution than the solution after the local search.

%
%
%\subsection{Description}
%
%As shown in listing \ref{hcSearch}, Hill Climber calculates an optimal solution based on the current network iteratively until no opration can be found to improve the network or the new network is only slightly better than the previous one. To find the best operation all possible operations get evaluated and the best one is returned. The algorithm stops, when it finds a local optima.
%
%\begin{lstlisting}[caption={HillClimbers search algorithm},label=hcSearch]
%double fScore = calcScore(bayesNet);
%Operation oOperation = getOptimalOperation(bayesNet, instances);
%while ((oOperation != null) && (oOperation.m_fScore > fScore)) {
%	performOperation(bayesNet, instances, oOperation);
%	fScore = oOperation.m_fScore;
%	oOperation = getOptimalOperation(bayesNet, instances);
%}  
%\end{lstlisting}


%
%
%\subsection{Test Results}
%
%Two data sets are used to test the search algorithm. These are x and y from the assignment 1.
%
%\todo{insert results of test}
%
%As shown in the result listing, \todo{describe result} ....

%
%

%\subsection{Description}
%
%Like the Hill Climber, our improved search algorithm introduces an initial Naive Bayes like network where each attribute has the classification attribute as parent.
%
%Then the Hill Climber algorithm is executed to find a local optima. To find other local optima, our search algorithm iteratively selects a neighbor of the found network and uses it as initial network for the Hill Climber to hopefully find better local optima than the first one.
%
% The intial network we use in each iteration is a random neighbor of the previously found one. It does not matter if it is better or worse.
%
%If the Hill Climber finds a better network it is stored as the current best found network and if the seacher cannot find a better network after a given number of iterations, the algorithm stops and yields the best solution found.



%
%
%%
%\section{Comparison}
%
%TODO:
%* used datasets
%* accuracy results
%* runtime/performance


\bibliography{references}


\end{document}
%------------------------------------------------


