%x%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Short Sectioned Assignment
% LaTeX Template
% Version 1.0 (5/5/12)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Frits Wenneker (http://www.howtotex.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%   PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[paper=a4, fontsize=11pt]{scrartcl} % A4 paper and 11pt font size

\usepackage{color}
\usepackage{float}
\usepackage{placeins}
\usepackage{natbib}
% philipp: added for code listings
\usepackage{listings}
\lstset{
    captionpos=b,
    basicstyle=\small
}
\newcommand{\Hilight}{\makebox[0pt][l]{\color{red}\rule[-4pt]{0.65\linewidth}{14pt}}}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{morefloats}
\bibliographystyle{plain}

\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
\usepackage{fourier} % Use the Adobe Utopia font for the document - comment this line to return to the LaTeX default
\usepackage[english]{babel} % English language/hyphenation
\usepackage{amsmath,amsfonts,amsthm} % Math packages

\usepackage{lipsum} % Used for inserting dummy 'Lorem ipsum' text into the template
\usepackage{todonotes}

\usepackage{sectsty} % Allows customizing section commands
\allsectionsfont{\centering \normalfont\scshape} % Make all sections centered, the default font and small caps

\usepackage{fancyhdr} % Custom headers and footers
\pagestyle{fancyplain} % Makes all pages in the document conform to the custom headers and footers
\fancyhead{} % No page header - if you want one, create it in the same way as the footers below
\fancyfoot[L]{} % Empty left footer
\fancyfoot[C]{} % Empty center footer
\fancyfoot[R]{\thepage} % Page numbering for right footer
% \renewcommand{\thesection}{Data Set \arabic{section}}
\renewcommand{\headrulewidth}{0pt} % Remove header underlines
\renewcommand{\footrulewidth}{0pt} % Remove footer underlines
\setlength{\headheight}{13.6pt} % Customize the height of the header

\numberwithin{equation}{section} % Number equations within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{figure}{section} % Number figures within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{table}{section} % Number tables within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)

\setlength{\parskip}{\baselineskip}%
\setlength\parindent{0pt} % Removes all indentation from paragraphs - comment this line for an assignment with lots of text
%----------------------------------------------------------------------------------------
%   TITLE SECTION
%----------------------------------------------------------------------------------------

\newcommand{\horrule}[1]{\rule{\linewidth}{#1}} % Create horizontal rule command with 1 argument of height

\title{ 
\normalfont \normalsize 
\textsc{Vienna University of Technology} \\ [25pt] % Your university, school and/or department name(s)
\horrule{0.5pt} \\[0.4cm] % Thin top horizontal rule
\huge Experiments in Machine Learning 2 \\ % The assignment title
\horrule{2pt} \\[0.5cm] % Thick bottom horizontal rule
}

\author{Benjamin Kiesl \and Philipp Steinwender \and Robert Sch\"{a}fer} % Your name

\date{\normalsize\today} % Today's date or a custom date

\begin{document}

\maketitle % Print the title

%----------------------------------------------------------------------------------------
%   PROBLEM 1
%----------------------------------------------------------------------------------------

\tableofcontents


%
%
%
\section{Preface}

This assignment is about extending a Bayes Network Classifier by improving an existing search heuristic in Weka. We chose the Hill Climber search algorithm for an implementation of an iterative local search algorithm.

%
%
%
\section{Setup}

There is a R script to execute the searcher in the handed in package at \texttt{R/bayesNetTest.R}. It loads Weka, builds the classifier, runs it and evaluates the model. It also shows the performance tables and the differences of the original Hill Climber implementation and our improved one. To run the script the following Tools need to be installed: R with the packages RWeka and rJava, Java (JRE).

The used package RWeka is an interface in R that wraps the Weka framework and has a really good documentation.

%
%
%\section{Weka-Implementation of the Bayesian Network}
%
%
%First we take a look at the BayesNet classifier implementation. When the classifier is startet, first thing is to generate the classification model. Therefore the BayesNet classifier initializes an adjacency matrix (\texttt{initStructure()}) where each attribute has an array of parents. These arrays are empty after initialization. 
%
%%
%%\begin{lstlisting}[caption={Initialization of BayesNet},label=initStructure]
%%...
%%// initialize parent sets to have arrow from classifier node to
%%// each of the other nodes
%%for (int iAttribute = 0; iAttribute < instances.numAttributes(); iAttribute++) {
%%    if (iAttribute != iClass) {
%%        bayesNet.getParentSet(iAttribute).addParent(iClass, instances);
%%    }
%%}
%%...
%%\end{lstlisting}
%
%The default Weka's SearchAlgorithm initializes the network like a Naive Bayes network. Listing \ref{initStructure} shows how we can use the adjacency matrix of BayesNet. Where \texttt{BayesNet} is the classifier that holds the network datastructure and \texttt{getParentSet(n)} returns the \texttt{nth} row in the adjacency matrix.

%
%
%
\section{Weka-Implementation of Hill Climber}

We chose to implement an iterated local search which is based on the pre-implemented Hill Climber algorithm in Weka. There exist implementations as local score based algorithm and as global score based algorithm.  We chose the global score based algorithm because this is better suited to iterated local search (for the perturbation and comparison with other solutions). The existing Hill Climber algorithm does the following:

\begin{enumerate}
	\item Create a start solution: One can choose between an empty network which contains no arcs or a Naive Bayes network as start solution. In the Naive Bayes network the classification node is the parent of all other attribute nodes.
	\item Local search: The algorithm chooses always the best solution from its neighborhood. The neighborhood is defined as all solutions which are accessible via the following operations:
		\begin{itemize}
			\item Add a new arc.
			\item Delete an existing arc.
			\item Reverse an existing arc.
		\end{itemize}
		As normal in local search the neighborhood is exploited until a local optimum is reached. The best solution is then returned as the Bayesian network.
\end{enumerate}

\newpage

\section{Classification results of the pre-implemented Hill Climber}


\begin{lstlisting}[caption={Classification result with default Hill Climber},label=listingDefault]
"ADULT DATA SET"
"Original Hill Climber classifier"
              pctCorrect             pctIncorrect          pctUnclassified 
              84.7025583               15.2974417                0.0000000 
                   kappa        meanAbsoluteError     rootMeanSquaredError 
               0.6078686                0.1722236                0.3319906 
   relativeAbsoluteError rootRelativeSquaredError 
              47.1008155               77.6450216 
\end{lstlisting}




\section{Improvement of the Hill Climber: Iterated Local Search}

We improved the existing Hill Climber algorithm by making an iterated local search out of it. The basic functionality is as follows:

\begin{itemize}
	\item Create a start solution.
	\item Apply local search to the start solution. The solution obtained is the new best solution.
	\item Repeat the following procedure until there is no new best solution for five iterations:
		\begin{itemize}
			\item Perturbate the current best solution.
			\item Apply local search to the perturbated solution.
			\item If the solution obtained after local search is better than the current best solution then this solution is the new best solution.
		\end{itemize}
\end{itemize}

It remains to describe what the Perturbation-procedure does: The perturbation-procedure modifies the given net by applying some number of random moves in the neighborhood. We implemented the search procedure in a way such that it is possible for the user to choose the number of random moves which are applied in every perturbation step. The standard value of random moves is three. Since it is not always possible to apply a move there are some tests to make sure that a random move is possible. This is necessary in the following cases:

\begin{itemize}
	\item The network doesn't contain an arc. (Deletion of an arc is not allowed/possible.)
	\item Every node has a maximum number of parents. (Addition of an arc is not allowed/possible.)
	\item Addition of an arc creates a cycle. (This test is pre-implemented in Weka.
	\item etc.
\end{itemize}

\section{Classification results of the iterated local search}



\begin{lstlisting}[caption={Classification result with iterated local search and Hill Climber},label=listingExtended]
"ADULT DATA SET"
"New iterated local search classifier"
              pctCorrect             pctIncorrect          pctUnclassified 
              84.8376893               15.1623107                0.0000000 
                   kappa        meanAbsoluteError     rootMeanSquaredError 
               0.6114594                0.1711662                0.3298876 
   relativeAbsoluteError rootRelativeSquaredError 
              46.8116196               77.1531638 

\end{lstlisting}





\section{Comparison of the Hill Climber algorithm and the iterated local search}

One can see that the iterated local search does improve the results slightly the pre-implemented Hill-Climber algorithm. The perturbation normally leads to worse solutions and local search then finds a net which is close to Naive Bayes or pretty similar to it so we don't get a better solution than the solution after the local search.

\begin{lstlisting}[caption={Comparison of classification results},label=listingComparison]
"Difference of details"
              pctCorrect             pctIncorrect          pctUnclassified 
             0.135130985             -0.135130985              0.000000000 
                   kappa        meanAbsoluteError     rootMeanSquaredError 
             0.003590769             -0.001057442             -0.002103060 
   relativeAbsoluteError rootRelativeSquaredError 
            -0.289195892             -0.491857758 
"Difference of confusion matrices"
       predicted
        >50K <=50K
  >50K    26   -26
  <=50K  -18    18

\end{lstlisting}


\bibliography{references}


\end{document}
%------------------------------------------------


