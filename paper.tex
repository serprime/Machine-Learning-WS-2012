%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Short Sectioned Assignment
% LaTeX Template
% Version 1.0 (5/5/12)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Frits Wenneker (http://www.howtotex.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%   PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[paper=a4, fontsize=11pt]{scrartcl} % A4 paper and 11pt font size

% philipp: added for code listings
\usepackage{listings}

\usepackage{natbib}
\usepackage{listings}
\bibliographystyle{plain}

\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
\usepackage{fourier} % Use the Adobe Utopia font for the document - comment this line to return to the LaTeX default
\usepackage[english]{babel} % English language/hyphenation
\usepackage{amsmath,amsfonts,amsthm} % Math packages

\usepackage{lipsum} % Used for inserting dummy 'Lorem ipsum' text into the template
\usepackage{todonotes}

\usepackage{sectsty} % Allows customizing section commands
\allsectionsfont{\centering \normalfont\scshape} % Make all sections centered, the default font and small caps

\usepackage{fancyhdr} % Custom headers and footers
\pagestyle{fancyplain} % Makes all pages in the document conform to the custom headers and footers
\fancyhead{} % No page header - if you want one, create it in the same way as the footers below
\fancyfoot[L]{} % Empty left footer
\fancyfoot[C]{} % Empty center footer
\fancyfoot[R]{\thepage} % Page numbering for right footer
% \renewcommand{\thesection}{Data Set \arabic{section}}
\renewcommand{\headrulewidth}{0pt} % Remove header underlines
\renewcommand{\footrulewidth}{0pt} % Remove footer underlines
\setlength{\headheight}{13.6pt} % Customize the height of the header

\numberwithin{equation}{section} % Number equations within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{figure}{section} % Number figures within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{table}{section} % Number tables within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)

\setlength\parindent{0pt} % Removes all indentation from paragraphs - comment this line for an assignment with lots of text

%----------------------------------------------------------------------------------------
%   TITLE SECTION
%----------------------------------------------------------------------------------------

\newcommand{\horrule}[1]{\rule{\linewidth}{#1}} % Create horizontal rule command with 1 argument of height

\title{ 
\normalfont \normalsize 
\textsc{Vienna University of Technology} \\ [25pt] % Your university, school and/or department name(s)
\horrule{0.5pt} \\[0.4cm] % Thin top horizontal rule
\huge Experiments in Machine Learning \\ % The assignment title
\horrule{2pt} \\[0.5cm] % Thick bottom horizontal rule
}

\author{Benjamin Kiesl \and Philipp Steinwender \and Robert Sch\"{a}fer} % Your name

\date{\normalsize\today} % Today's date or a custom date

\begin{document}

\maketitle % Print the title

%----------------------------------------------------------------------------------------
%   PROBLEM 1
%----------------------------------------------------------------------------------------

\todo[inline]{defined:

validation: cross-validation with 10 for big data sets (9 training, 1 test), 3 for the small echo cardio set.
}


\todo[inline]{Chose 5 different classifiers, from at least 3 different types
of learning algorithms

Argue \& justify choice (part of grading...)

Experiment with different parameter settings - And report on it !

Compare results among classifiers and datasets

Evaluate effect of pre-processing (e.g. different strategies
for missing values)
}

\section{Echocardiogram data}

The Echocardiogram data set consists of 132 entries with 13 attributes respectively and has 132 missing values in total. Originally, these entries referred to patients that suffered from a heart attack at some point in the past \cite{uci-repo}. The prediction task is to determine if the patient will survive at least one year.

\subsection{Preprocessing}
The name attribute of the patients was replaced by a dummy for all entries. For this reason, this attribute can be entirely removed. Another unnecessary attribute is ``group'' which is just meaningless and can be ignored \cite{uci-repo}. Another derived attribute is ``mult'' and can be ignored as well.

Some of the patients survived less than 12 months even if they were still alive at the end of the survey. In this case these patients joined the survey too recently to be followed at least one year. To make a reasonable prediction if a patient will survive at least one year, those entries have to be left out for the training data set because they would irritate any learning algorithm. \todo[inline]{maybe, give an example to show that those patients confuse a machine learning algorithm} 

Yet, there are some entries, which can be wiped out from the whole data set as well:

\missingfigure{Show the crappy entry with the 76 years old patient who has only null values}

So long, these preprocessing tasks are just removing attributes (column-wise) or removing certain entries (row-wise), depending on their values.

Unfortunately, there is another peculiarity with the actual prediction attribute ``alive-at-1'', here is the official description:
\begin{quotation} ``Boolean-valued. Derived from the first two attributes. 0 means patient was either dead after 1 year or had been followed for less than 1 year.  1 means patient was alive at 1 year.''\cite{uci-repo} \end{quotation}
De facto, the values don't fit to this description. The attribute is $1$ if the patient was followed less than one year and $0$ otherwise. Very astonishing: Even if the attribute is supposed to be a derived attribute, there are several \texttt{null} values, making the corresponding values useless for training data.                                                                

\missingfigure{Show the crappy null valued entries}
The only way to amend this is to add the prediction value by hand: At first, all patients who joined the survey too recently, have to be left out (i.e. survived less than one year but still alive). Then, the ``alive-at-one'' attribute has to be overwritten with the correct value (i.e. $1$ if the patient survived at least one year).
\todo{Compare results with/without preprocessing}

After preprocessing only 95 of 132 instances remain and it is still a set full of gaps with 45 missing values in total. After preprocessing it is also a very imbalanced data set for machine learning: Only 4 patients died within 12 months, a ratio of 1 to 24. As the documentation of the UCI repository suggests ``The most difficult part of this problem is correctly predicting that the patient will NOT survive. (Part of the difficulty seems to be the size of the data set.)''\cite{uci-repo}. Indeed this is rather difficult to predict with only four valid instances. 

\subsection{Decision Tree}
%------------------------------------------------
On the first glimpse, it is an easy task for a decision based algorithm:

\begin{lstlisting}
J48 pruned tree
------------------

survival <= 12
|   survival <= 11: 0 (4.0)
|   survival > 11: 1 (4.0)
survival > 12: 1 (87.0)
\end{lstlisting}

That's correct, if the patient survived $ <= 12 $ months, he survived at least 12 month. What a statement! 

The only remarkable thing is, that the J48 implementation under default settings seems to enforce a certain depth of the tree, even if the splitting on the same attribute don't make any sense ($ 11 $ is yet a smaller number than $ 12 $, isn't it?). E.g. choosing a DecisionStump results in the expected single splitting: 
\begin{lstlisting}
Class distributions
survival <= 11.5
0   1   
1.0 0.0 
survival > 11.5
0   1   
0.0 1.0 
survival is missing
0   1   
0.042105263157894736    0.9578947368421052
\end{lstlisting}
Since there is no missing value for the survival attribute, this branch can be ignored.

But allowing the machine learning algorithms to see the origin attributes from which we derived the prediction attribut leads to absurdity, since the actual prediction task is to say whether a patient will survive one year after suffering a heart attack or not \emph{depending on the medical characteristics}.


\subsection{re-preprocessed the data set}

As mentioned in the description of the dataset on the UCI repository site we can remove the attributes ``group'', ``name'' and ``mult'' as they do not have any meaning for the prediction attribute.

We also have to remove the ``survival'' and ``still-alive'' attributes, as the prediction attribute derives from them. Otherwise, a classification algorithm could predict all instances by taking into account only these two attributes.

A further refinement of the data set has been done by deleting all instances, where the outcome was unknown, as this cannot be used for training.

\subsection{Decision Tree on re-preprocessed data}

The J48 Decision Tree shows a more useful tree on the re-preprocessed data:
\begin{lstlisting}
J48 pruned tree

epss <= 13100: 1 (90.63/1.95)
epss > 13100
|   fractional-shortening <= 190: 1 (2.25/0.05)
|   fractional-shortening > 190: 0 (2.11/0.11)
\end{lstlisting}

The documentation of the data set explains the two attributes as follows:
\begin{quote}
``epss -- E-point septal separation, another measure of contractility. Larger numbers are increasingly abnormal.''
\end{quote}
\begin{quote}
`` fractional-shortening -- a measure of contracility around the heart lower numbers are increasingly abnormal''
\end{quote}

Only the rule for the fractional shortening should be the other way round, because a low fractional shortening is a rather pathological sign. Although the decision tree has a high precision of almost 95\% it performs very imprecise, because none of the patients who died after one year are correctly classified:

\begin{lstlisting}
=== Confusion Matrix ===

  a  b   <-- classified as
  0  4 |  a = 0
  1 90 |  b = 1
    
\end{lstlisting}

Using an unpruned tree doesn't improve the results:
\begin{lstlisting}
J48 unpruned tree


pericardial-effusion = 0
|   epss <= 13100: 1 (76.62/1.95)
|   epss > 13100
|   |   epss <= 15600: 0 (2.19/0.16)
|   |   epss > 15600: 1 (2.19/0.03)
pericardial-effusion = 1: 1 (14.0)
\end{lstlisting}
Here, the existence of a pericardial effusion (fluid around the heart) is assumed to be a healthy characteristic, but it is not. Accordingly, the precision of the J48 decreases from 95\% to 88\%.
\section{Adult data}

The Adult or Census Income data sets contains 48842 instances of information about persons and if they earn more than 50.000 a year. The data set has 14 attributes and missing values.

The instances origin from the 1994 Census Database and were extracted by Barry Becker. For having a clean data set, only records with age above 16 with minimium one working hour per week have been selected from the database.

\subsection{Preprocessing}

We can remove the final weight -- but we have to find out what it means !!.

We can also remove the $education-num$ attribute as it is only a numeric representation of the education attribute.

At first we try to discretize all numeric attributes with 10 bins of equal ranges.




\subsection{Decision Tree}




\subsection{Random Forrest}




\subsection{Support Vector Machine}




\subsection{Naive Bayes}

We use 66\% of the data set for training and the rest for evaluation.

For the first run, we use the default setting and get the following result:

\begin{verbatim}
Correctly Classified Instances        9104               82.2329 %
Incorrectly Classified Instances      1967               17.7671 %
Total Number of Instances            11071     

Precision   Recall  F-Measure  Class
 0.592       0.786   0.675      >50K
 0.927       0.834   0.878      <=50K
\end{verbatim}

Second run: we use the discretizer with 20 bin.

\begin{verbatim}
Correctly Classified Instances        9287               83.8858 %
Incorrectly Classified Instances      1784               16.1142 %
Total Number of Instances            11071     

=== Detailed Accuracy By Class ===

Precision   Recall  F-Measure  Class
 0.621       0.805   0.701      >50K
 0.934       0.849   0.89       <=50K
\end{verbatim}

As expected, the result is slightly better.

A better result should be possible if we let the dicretizer split the values in part of nearly equal size. The result:

\begin{verbatim}
Correctly Classified Instances        9179               82.9103 %
Incorrectly Classified Instances      1892               17.0897 %
Total Number of Instances            11071     

Precision   Recall  F-Measure  Class
0.602        0.805   0.689      >50K
0.933        0.836   0.882      <=50K
\end{verbatim}

It got worse. \todo{why does discretizer with equal bin size not give a better result?}

The Naive Bayes does not have a lot of settings. It use a built in supervised discretizer or a kernel estimator. But the result is always the same with around 80\% correctly classified instances in the test run.

\subsection{k-Nearest}


\section{Contraceptive Method Choice Data}

The Contraceptive Method Choice Data Set consists of information about the demographic and socio-economic characteristics of married women. The task is to predict which kind of contraceptive methods a woman uses. These are categorized as no use, short term or long term.

\subsection{Preprocessing}

The attributes are already in a numerical and categorical form, whereby the categories are encoded in an ordinal form. These go from 1 to 4 where 1 is low/bad and 4 is high/good.

\todo{check if ordinal is the correct term when i mean, that the attributes are encoded in numbers, where 1<2, 1<4, 4>2, ...}


\subsection{Desicion Tree}
\subsection{Random Forrest}
\subsection{Naive Bayes}
\subsection{Support VectorMachine}
\subsection{k-Nearest}



\bibliography{references}


\end{document}
%------------------------------------------------


