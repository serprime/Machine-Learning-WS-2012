%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Short Sectioned Assignment
% LaTeX Template
% Version 1.0 (5/5/12)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Frits Wenneker (http://www.howtotex.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%   PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[paper=a4, fontsize=11pt]{scrartcl} % A4 paper and 11pt font size

% philipp: added for code listings
\usepackage{listings}

\usepackage{natbib}
\usepackage{listings}
\bibliographystyle{plain}

\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
\usepackage{fourier} % Use the Adobe Utopia font for the document - comment this line to return to the LaTeX default
\usepackage[english]{babel} % English language/hyphenation
\usepackage{amsmath,amsfonts,amsthm} % Math packages

\usepackage{lipsum} % Used for inserting dummy 'Lorem ipsum' text into the template
\usepackage{todonotes}

\usepackage{sectsty} % Allows customizing section commands
\allsectionsfont{\centering \normalfont\scshape} % Make all sections centered, the default font and small caps

\usepackage{fancyhdr} % Custom headers and footers
\pagestyle{fancyplain} % Makes all pages in the document conform to the custom headers and footers
\fancyhead{} % No page header - if you want one, create it in the same way as the footers below
\fancyfoot[L]{} % Empty left footer
\fancyfoot[C]{} % Empty center footer
\fancyfoot[R]{\thepage} % Page numbering for right footer
% \renewcommand{\thesection}{Data Set \arabic{section}}
\renewcommand{\headrulewidth}{0pt} % Remove header underlines
\renewcommand{\footrulewidth}{0pt} % Remove footer underlines
\setlength{\headheight}{13.6pt} % Customize the height of the header

\numberwithin{equation}{section} % Number equations within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{figure}{section} % Number figures within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{table}{section} % Number tables within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)

\setlength\parindent{0pt} % Removes all indentation from paragraphs - comment this line for an assignment with lots of text

%----------------------------------------------------------------------------------------
%   TITLE SECTION
%----------------------------------------------------------------------------------------

\newcommand{\horrule}[1]{\rule{\linewidth}{#1}} % Create horizontal rule command with 1 argument of height

\title{ 
\normalfont \normalsize 
\textsc{Vienna University of Technology} \\ [25pt] % Your university, school and/or department name(s)
\horrule{0.5pt} \\[0.4cm] % Thin top horizontal rule
\huge Experiments in Machine Learning \\ % The assignment title
\horrule{2pt} \\[0.5cm] % Thick bottom horizontal rule
}

\author{Benjamin Kiesl \and Philipp Steinwender \and Robert Sch\"{a}fer} % Your name

\date{\normalsize\today} % Today's date or a custom date

\begin{document}

\maketitle % Print the title

%----------------------------------------------------------------------------------------
%   PROBLEM 1
%----------------------------------------------------------------------------------------

\todo[inline]{defined:

validation: cross-validation with 10 for big data sets (9 training, 1 test), 3 for the small echo cardio set.
}


\todo[inline]{Chose 5 different classifiers, from at least 3 different types
of learning algorithms

Argue \& justify choice (part of grading...)

Experiment with different parameter settings - And report on it !

Compare results among classifiers and datasets

Evaluate effect of pre-processing (e.g. different strategies
for missing values)
}

\section{Echocardiogram data}

The Echocardiogram data set consists of 132 entries with 13 attributes respectively and has 132 missing values in total. Originally, these entries referred to patients that suffered from a heart attack at some point in the past \cite{uci-repo}. The prediction task is to determine if the patient will survive at least one year.

\subsection{Preprocessing}
The name attribute of the patients was replaced by a dummy for all entries. For this reason, this attribute can be entirely removed. Another unnecessary attribute is ``group'' which is just meaningless and can be ignored \cite{uci-repo}. Another derived attribute is ``mult'' and can be ignored as well.

Some of the patients survived less than 12 months even if they were still alive at the end of the survey. In this case these patients joined the survey too recently to be followed at least one year. To make a reasonable prediction if a patient will survive at least one year, those entries have to be left out for the training data set because they would irritate any learning algorithm. \todo[inline]{maybe, give an example to show that those patients confuse a machine learning algorithm} 

Yet, there are some entries, which can be wiped out from the whole data set as well:

\missingfigure{Show the crappy entry with the 76 years old patient who has only null values}

So long, these preprocessing tasks are just removing attributes (column-wise) or removing certain entries (row-wise), depending on their values.

Unfortunately, there is another peculiarity with the actual prediction attribute ``alive-at-1'', here is the official description:
\begin{quotation} ``Boolean-valued. Derived from the first two attributes. 0 means patient was either dead after 1 year or had been followed for less than 1 year.  1 means patient was alive at 1 year.''\cite{uci-repo} \end{quotation}
De facto, the values don't fit to this description. The attribute is $1$ if the patient was followed less than one year and $0$ otherwise. Very astonishing: Even if the attribute is supposed to be a derived attribute, there are several \texttt{null} values, making the corresponding values useless for training data.                                                                

We thus inferred the prediction attribute ``alive-at-1'' as explained in the description of the whole dataset: For all patients where the months of survival are greater than or equal to 12 we set the ``alive-at-1''-attribute to 1, otherwise we set it to 0.

The only thing which remained then was to a

\todo{Compare results with/without preprocessing}

\subsection{Decision Tree}
%------------------------------------------------
On the first glimpse, it is an easy task for a decision based algorithm:

\begin{lstlisting}
J48 pruned tree
------------------

survival <= 12
|   survival <= 11: 0 (4.0)
|   survival > 11: 1 (4.0)
survival > 12: 1 (87.0)
\end{lstlisting}

That's correct, if the patient survived $ <= 12 $ months, he survived at least 12 month. What a statement! 

The only remarkable thing is, that the J48 implementation under default settings seems to enforce a certain depth of the tree, even if the splitting on the same attribute don't make any sense ($ 11 $ is yet a smaller number than $ 12 $, isn't it?). E.g. choosing a DecisionStump results in the expected single splitting: 
\begin{lstlisting}
Class distributions
survival <= 11.5
0   1   
1.0 0.0 
survival > 11.5
0   1   
0.0 1.0 
survival is missing
0   1   
0.042105263157894736    0.9578947368421052
\end{lstlisting}
Since there is no missing value for the survival attribute, this branch can be ignored.

But allowing the machine learning algorithms to see the origin attributes from which we derived the prediction attribut leads to absurdity, since the actual prediction task is to say whether a patient will survive one year after suffering a heart attack or not \emph{depending on the medical characteristics}.


\subsection{re-preprocessed the data set}

As mentioned in the description of the dataset on the UCI repository site we can remove the attributes ``group'', ``name'' and ``mult'' as they do not have any meaning for the prediction attribute.

We also have to remove the ``survival'' and ``still-alive'' attributes, as the prediction attribute derives from them. Otherwise, a classification algorithm could predict all instances by taking into account only these two attributes.

A further refinement of the data set has been done by deleting all instances, where the outcome was unknown, as this cannot be used for training.

\subsection{Decision Tree on re-preprocessed data}

The J48 Decision Tree shows a more useful tree on the re-preprocessed data:
\begin{lstlisting}
J48 pruned tree

epss <= 13100: 1 (90.63/1.95)
epss > 13100
|   fractional-shortening <= 190: 1 (2.25/0.05)
|   fractional-shortening > 190: 0 (2.11/0.11)
\end{lstlisting}

The documentation of the data set explains the two attributes as follows:
\begin{quote}
``epss -- E-point septal separation, another measure of contractility. Larger numbers are increasingly abnormal.''
\end{quote}
\begin{quote}
`` fractional-shortening -- a measure of contracility around the heart lower numbers are increasingly abnormal''
\end{quote}

Only the rule for the fractional shortening should be the other way round, because a low fractional shortening is a rather pathological sign. Although the decision tree has a high precision of almost 95\% it performs very imprecise, because none of the patients who died after one year are correctly classified:

\begin{lstlisting}
=== Confusion Matrix ===
  a  b   <-- classified as
  0  4 |  a = 0
  1 90 |  b = 1
    
\end{lstlisting}

Using an unpruned tree doesn't improve the results:
\begin{lstlisting}
J48 unpruned tree


pericardial-effusion = 0
|   epss <= 13100: 1 (76.62/1.95)
|   epss > 13100
|   |   epss <= 15600: 0 (2.19/0.16)
|   |   epss > 15600: 1 (2.19/0.03)
pericardial-effusion = 1: 1 (14.0)
\end{lstlisting}
Here, the existence of a pericardial effusion (fluid around the heart) is assumed to be a healthy characteristic, but it is not. Accordingly, the precision of the J48 decreases from 95\% to 88\%.

\subsubsection{Random Forest}
Let's see if a random forest in place of a simple decision tree can make better result out of the echocardiogram data set. With default settings, i.e. default number of features \todo[inline]{calculate!} and 10 random trees with unlimited tree depth, the random forest shows us a rather disappointing result:

\begin{lstlisting}
=== Confusion Matrix ===
  a  b   <-- classified as
  0  4 |  a = 0
  2 91 |  b = 1
\end{lstlisting}
And the statistical measures:
\begin{lstlisting}
=== Detailed Accuracy By Class ===
                 TP Rate  FP Rate  Precision  Recall  F-Measure    Class
                 0        0.022    0          0       0            0
                 0.978    1        0.958      0.978   0.968        1
Weighted Avg.    0.938    0.96     0.918      0.938   0.928     
\end{lstlisting}
\paragraph{Parameter: Number of trees}
\todo[inline]{proper line break here}
Reducing the number of the trees continuously decreases the quality of the model, as shown in the accuracy details for 5 random trees:
\begin{lstlisting}
=== Detailed Accuracy By Class ===
                 TP Rate  FP Rate  Precision  Recall  F-Measure    Class
                 0        0.032    0          0       0            0  
                 0.968    1        0.957      0.968   0.963        1   
Weighted Avg.    0.928    0.96     0.918      0.928   0.923    
\end{lstlisting}
.. and for a single random tree:
\begin{lstlisting}
=== Detailed Accuracy By Class ===
                 TP Rate  FP Rate  Precision  Recall  F-Measure    Class
                 0        0.054    0          0       0            0  
                 0.946    1        0.957      0.946   0.951        1   
Weighted Avg.    0.907    0.961    0.917      0.907   0.912    
\end{lstlisting}
Reducing the number of the trees increases the number of false positives but doesn't catch any true positives.
\paragraph{Limiting the tree-depth}
Limitations in the depths of the random trees has also almost no effect on the accuracy of random forests on this data set. The only change to the outcome can be observed as soon the depth of the trees is limited to 6:
\begin{lstlisting}
    === Confusion Matrix ===
    a  b   <-- classified as
    0  4 |  a = 0
    1 92 |  b = 1
\end{lstlisting}
Only one false positive has gone. Not really a remarkable change in the quality. This does not change if the tree-depth is further reduced.
\paragraph{Increasing the number of features}
Increasing the number of features chosen by each random tree seems to has also a low effect on the quality, because until a certain border of 5 chosen features the outcome is exactly the same as it was with default settings. But if we configure the random forests with 6 features we can spot our first true positive:
\begin{lstlisting}
=== Confusion Matrix ===
  a  b   <-- classified as
  1  3 |  a = 0
  2 91 |  b = 1
\end{lstlisting}
Which gives us the best achievable accuracy that we can reach with decision based algorithms:
\begin{lstlisting}
=== Detailed Accuracy By Class ===
                TP Rate  FP Rate  Precision  Recall  F-Measure   Class
                0.25     0.022    0.333      0.25    0.286       0
                0.978    0.75     0.968      0.978   0.973       1
Weighted Avg.   0.948    0.72     0.942      0.948   0.945      
\end{lstlisting}
\section{Adult data}

The Adult or Census Income data sets contains 48842 instances of information about persons and if they earn more than 50.000 a year. The data set has 14 attributes and missing values.

The instances origin from the 1994 Census Database and were extracted by Barry Becker. For having a clean data set, only records with age above 16 with minimium one working hour per week have been selected from the database.

\subsection{Preprocessing}

We can remove the final weight -- but we have to find out what it means !!.

We can also remove the $education-num$ attribute as it is only a numeric representation of the education attribute.

At first we try to discretize all numeric attributes with 10 bins of equal ranges.




\subsection{Decision Tree}
Using a decision tree yields a precision about 86\%. Varying the parameters for this algorithm influences the quality of the results only slightly, as the following examples illustrate.

First of all, with the default settings of the J48 implementation of weka, i.e. using pruning with a confidence factor of 0.25 together with MDLCorrection and Subtree raising and at least 2 instances per leaf of the tree, we get the following summary of the quality:
 
\begin{lstlisting}
=== Detailed Accuracy By Class ===
                TP Rate  FP Rate  Precision  Recall  F-Measure  Class
                0.615    0.062    0.759      0.615   0.68       >50K
                0.938    0.385    0.885      0.938   0.911      <=50K
Weighted Avg.   0.86     0.307    0.854      0.86    0.855      
\end{lstlisting}

The decision tree that was used for this run has a size of 413 and has 311 leaves.

\paragraph{Raising the confidence factor}
Subsequently increasing the confidence factor, which result in less pruning, leads to a continuous worse quality. If we set the confidence factor to 0.5 we get the following outcome:
\begin{lstlisting}
=== Detailed Accuracy By Class ===
                TP Rate  FP Rate  Precision  Recall  F-Measure  Class
                0.632    0.071    0.738      0.632   0.681      >50K
                0.929    0.368    0.888      0.929   0.908      <=50K
Weighted Avg.   0.857    0.296    0.852      0.857   0.854      
\end{lstlisting}
.. for a confidence factor of 0.75 we get the following:
\begin{lstlisting}
=== Detailed Accuracy By Class ===
                TP Rate  FP Rate  Precision  Recall  F-Measure  Class
                0.655    0.094    0.689      0.655   0.672      >50K
                0.906    0.345    0.892      0.906   0.899      <=50K
Weighted Avg.   0.846    0.285    0.843      0.846   0.844     
\end{lstlisting}

\paragraph{Other parameters}
Varying other parameters has a similar effect like the one mentioned above. Either the quality of the J48 algorithm decreases a little or it stays the same, thus never exceeding an overall quality of 86\%. To sum it up: Using an unpruned tree yields the same quality as a tree with a confidence factor of 0.75. Smoothing the counts at the leaves with laplace has no effect. Either without MDLcorrection or without Subtree raising during pruning, in both cases the precision falls from 86\% to 85.5\%. If we rise the minimum of instances per leaves, e.g. to 20 instances and 50 instances, the precision falls from 86\% to 85\% and 84\$ respectively.

\subsection{Random Forrest}

With default settings, a random forest achieves an accuracy as follows:
\begin{lstlisting}
=== Detailed Accuracy By Class ===
                TP Rate  FP Rate  Precision  Recall  F-Measure  Class
                0.615    0.108    0.644      0.615   0.629      >50K
                0.892    0.385    0.88       0.892   0.886      <=50K
Weighted Avg.   0.825    0.318    0.823      0.825   0.824     
\end{lstlisting}
which is surprisingly a worse result than from the J48 algorithm. 

\paragraph{Number of Random Trees}
As expected, reducing the number of random trees reduces the quality, with 5 random trees the precision lays at 82.6\% with only 3 random trees it lays at 82.3\%. Interestingly, the quality gets higher if we take even more than 10 random trees into account: With 15 trees, the precision is at 83.2\% and with 25 random trees it is at 83.3\%.

\paragraph{Limitations of the maximum depth of the trees}
We have a sign for overfitting under default settings: Reducing the maximum depth has a positive effect on the quality. From size 20 over 10 until the size of 7 the precision of the random forest rises until 85\%. Reducing the depth even further lets the precision fall again. Here is the detailed accuracy for a random forest which can grow only to a height of 6:

\begin{lstlisting}
=== Detailed Accuracy By Class ===
                TP Rate  FP Rate  Precision  Recall  F-Measure  Class
                0.606    0.064    0.749      0.606   0.67       >50K
                0.936    0.394    0.882      0.936   0.908      <=50K
Weighted Avg.   0.856    0.314    0.85       0.856   0.851     
\end{lstlisting}
We can push the accuracy even further, if we take more random trees, e.g. twenty:
\begin{lstlisting}
=== Detailed Accuracy By Class ===
                TP Rate  FP Rate  Precision  Recall  F-Measure  Class
                0.593    0.058    0.763      0.593   0.667      >50K
                0.942    0.407    0.879      0.942   0.909      <=50K
Weighted Avg.   0.858    0.323    0.851      0.858   0.851      
\end{lstlisting}

\paragraph{Number of features}
Increasing the number of features doesn't support the quality at all. The default number of features for this data set with 13 attributes is \todo[inline]{calculate!}. With a number of 5 randomly chosen features, the precision is yet at 85\% (with maximum depth of tree = 6) but with 10 randomly chosen features the precision falls back to 84.5\%.
\subsection{Support Vector Machine}




\subsection{Naive Bayes}

We use 66\% of the data set for training and the rest for evaluation.

For the first run, we use the default setting and get the following result:

\begin{verbatim}
Correctly Classified Instances        9104               82.2329 %
Incorrectly Classified Instances      1967               17.7671 %
Total Number of Instances            11071     

Precision   Recall  F-Measure  Class
 0.592       0.786   0.675      >50K
 0.927       0.834   0.878      <=50K
\end{verbatim}

Second run: we use the discretizer with 20 bin.

\begin{verbatim}
Correctly Classified Instances        9287               83.8858 %
Incorrectly Classified Instances      1784               16.1142 %
Total Number of Instances            11071     

=== Detailed Accuracy By Class ===

Precision   Recall  F-Measure  Class
 0.621       0.805   0.701      >50K
 0.934       0.849   0.89       <=50K
\end{verbatim}

As expected, the result is slightly better.

A better result should be possible if we let the dicretizer split the values in part of nearly equal size. The result:

\begin{verbatim}
Correctly Classified Instances        9179               82.9103 %
Incorrectly Classified Instances      1892               17.0897 %
Total Number of Instances            11071     

Precision   Recall  F-Measure  Class
0.602        0.805   0.689      >50K
0.933        0.836   0.882      <=50K
\end{verbatim}

It got worse. \todo{why does discretizer with equal bin size not give a better result?}

The Naive Bayes does not have a lot of settings. It use a built in supervised discretizer or a kernel estimator. But the result is always the same with around 80\% correctly classified instances in the test run.

\subsection{k-Nearest}


\section{Contraceptive Method Choice Data}

The Contraceptive Method Choice Data Set consists of information about the demographic and socio-economic characteristics of married women. The task is to predict which kind of contraceptive methods a woman uses. These are categorized as no use, short term or long term.

\subsection{Preprocessing}

The attributes are already in a numerical and categorical form, whereby the categories are encoded in an ordinal form. These go from 1 to 4 where 1 is low/bad and 4 is high/good.

\todo{check if ordinal is the correct term when i mean, that the attributes are encoded in numbers, where 1<2, 1<4, 4>2, ...}


\subsection{Desicion Tree}
\subsection{Random Forrest}
\subsection{Naive Bayes}
\subsection{Support VectorMachine}
\subsection{k-Nearest}



\bibliography{references}


\end{document}
%------------------------------------------------


