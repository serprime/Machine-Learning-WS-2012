%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Short Sectioned Assignment
% LaTeX Template
% Version 1.0 (5/5/12)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Frits Wenneker (http://www.howtotex.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%   PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[paper=a4, fontsize=11pt]{scrartcl} % A4 paper and 11pt font size

% philipp: added for code listings
\usepackage{listings}

\usepackage{natbib}
\usepackage{listings}
\bibliographystyle{plain}

\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
\usepackage{fourier} % Use the Adobe Utopia font for the document - comment this line to return to the LaTeX default
\usepackage[english]{babel} % English language/hyphenation
\usepackage{amsmath,amsfonts,amsthm} % Math packages

\usepackage{lipsum} % Used for inserting dummy 'Lorem ipsum' text into the template
\usepackage{todonotes}

\usepackage{sectsty} % Allows customizing section commands
\allsectionsfont{\centering \normalfont\scshape} % Make all sections centered, the default font and small caps

\usepackage{fancyhdr} % Custom headers and footers
\pagestyle{fancyplain} % Makes all pages in the document conform to the custom headers and footers
\fancyhead{} % No page header - if you want one, create it in the same way as the footers below
\fancyfoot[L]{} % Empty left footer
\fancyfoot[C]{} % Empty center footer
\fancyfoot[R]{\thepage} % Page numbering for right footer
% \renewcommand{\thesection}{Data Set \arabic{section}}
\renewcommand{\headrulewidth}{0pt} % Remove header underlines
\renewcommand{\footrulewidth}{0pt} % Remove footer underlines
\setlength{\headheight}{13.6pt} % Customize the height of the header

\numberwithin{equation}{section} % Number equations within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{figure}{section} % Number figures within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{table}{section} % Number tables within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)

\setlength\parindent{0pt} % Removes all indentation from paragraphs - comment this line for an assignment with lots of text

%----------------------------------------------------------------------------------------
%   TITLE SECTION
%----------------------------------------------------------------------------------------

\newcommand{\horrule}[1]{\rule{\linewidth}{#1}} % Create horizontal rule command with 1 argument of height

\title{ 
\normalfont \normalsize 
\textsc{Vienna University of Technology} \\ [25pt] % Your university, school and/or department name(s)
\horrule{0.5pt} \\[0.4cm] % Thin top horizontal rule
\huge Experiments in Machine Learning \\ % The assignment title
\horrule{2pt} \\[0.5cm] % Thick bottom horizontal rule
}

\author{Benjamin Kiesl \and Philipp Steinwender \and Robert Sch\"{a}fer} % Your name

\date{\normalsize\today} % Today's date or a custom date

\begin{document}

\maketitle % Print the title

%----------------------------------------------------------------------------------------
%   PROBLEM 1
%----------------------------------------------------------------------------------------

\todo[inline]{defined:

validation: cross-validation with 10 for big data sets (9 training, 1 test), 3 for the small echo cardio set.
}

\todo[inline]{Choose 5 different classifiers, from at least 3 different types
of learning algorithms

Argue \& justify choice (part of grading...)

Experiment with different parameter settings - And report on it !

Compare results among classifiers and datasets

Evaluate effect of pre-processing (e.g. different strategies
for missing values)
}

\section{Preface}

\paragraph{}For the first exercise 3 data sets were chosen on which five classification algorithms were tested to practice basic machine learning tasks and see how these algorithms behave with their various settings. For the elaboration of this paper the Machine Learning Software Weka was used. It offered all necessary algorithms.

\paragraph{}As data sets we chose as follows:
\begin{itemize}
\item Echo Cardiogram, a data set about heart attacks and if a patient survived one year after the attack.
\item Adult / Census Income, a data set about people and their basic information and if they earn more than 50k a year.
\item Contraceptive Method Choice, a data set about women and their environment and basic information and what contraceptive method they use.
\end{itemize}

\paragraph{}Five classification algorithms were chosen that we used on all data sets. Each chapter will also dicuss how the classifiers perform an the data and why.


% ------------------------------------
% E C H O   C A R D I O    S E T
% ------------------------------------


\section{Echo Cardiogram data}

\paragraph{}The Echocardiogram data set originally consists of 132 entries with 13 attributes respectively. The entries of the dataset refer to patients that suffered from a heart attack at some point in the past \cite{uci-repo}. The prediction task is to determine from several different attributes if the patient will survive at least one year, beginning from the day when the heart attack occured. The dataset contains missing values and there's a lot of other preprocessing steps (described later) which have to be done. The dataset is very small, especially after preprocessing and a further problem is that the number of patients which died within one year after their heart attack is very small (4) which makes the classification very hard. It is nevertheless an interesting question wether some classification algorithm can perform well on that data. Since it is very small we performed a 3-fold-cross-validation on this dataset.

\subsection{Preprocessing}
\paragraph{}The patient's attribute ``name'' was replaced by a dummy for all entries. For this reason, this attribute could be entirely removed. Other unnecessary attributes were ``group'' and ``mult'' which were also (as explained in the description) just meaningless and could thus be ignored \cite{uci-repo}. There were also two attributes ``wall-motion-score'' and ``wall-motion-index'' where the description of the dataset said that one should only use the ``wall-motion-index'' for classification issues and so ``wall-motion-score'' was also removed.

\paragraph{}The actual prediction attribute ``alive-at-1'' (denoting that a patient lived at least one year after having a heart attack) had to be inferred from two other attributes, namely ``survival'' (number of months that the test-person survived after the heart attack) and ``still-alive'' (a binary variable denoting if a patient is still alive at the time when the data was recorded). It is possible that the ``survival'' variable has a value lower than 12 but the ``still-alive'' variable is set to true, meaning that the test-person still lived. This is the case if the data for a test-person was recorded in the last 12 months and so this data cannot be used for the classification and was thus removed.

\paragraph{}It should be clear that the variables ``survival'' and ``still-alive'' should not be used for classification since they constitute the final prediction attribute.

\paragraph{}The last preprocessing step was the replacement of missing values (data imputation). Most of the data was complete but there were still some data records having missing values which we replaced by the mean values of the specific variables. One data record did contain completeley useless data and was thus removed (see at the picture below).

\paragraph{}

%So long, these preprocessing tasks are just removing attributes (column-wise) or removing certain entries (row-wise), depending on their values.
%
%Unfortunately, there is another peculiarity with the actual prediction attribute ``alive-at-1'', here is the official description:
%\begin{quotation} ``Boolean-valued. Derived from the first two attributes. 0 means patient was either dead after 1 year or had been followed for less than 1 year.  1 means patient was alive at 1 year.''\cite{uci-repo} \end{quotation}
%De facto, the values don't fit to this description. The attribute is $1$ if the patient was followed less than one year and $0$ otherwise. Very astonishing: Even if the attribute is supposed to be a derived attribute, there are several \texttt{null} values, making the corresponding values useless for training data.                                                                
%
%We thus inferred the prediction attribute ``alive-at-1'' as explained in the description of the whole dataset: For all patients where the months of survival are greater than or equal to 12 we set the ``alive-at-1''-attribute to 1, otherwise we set it to 0.

%The only thing which remained then was to a

\todo{Compare results with/without preprocessing}

\subsection{Decision Tree}
\todo{robert: do you want to keep this section? on the one hand it shows what happens if we run a desicion tree with attributes on which the the prediction attribute depends, but on the other hand we know we should not do that. please decide. i'm ok with both (philipp)}
%------------------------------------------------
On the first glimpse, it is an easy task for a decision based algorithm:

\begin{lstlisting}
J48 pruned tree
survival <= 12
|   survival <= 11: 0 (4.0)
|   survival > 11: 1 (4.0)
survival > 12: 1 (87.0)
\end{lstlisting}

\paragraph{}That's correct, if the patient survived $ <= 12 $ months, he survived at least 12 month. What a statement! 

\paragraph{}The only remarkable thing is, that the J48 implementation under default settings seems to enforce a certain depth of the tree, even if the splitting on the same attribute don't make any sense ($ 11 $ is yet a smaller number than $ 12 $, isn't it?). E.g. choosing a DecisionStump results in the expected single splitting: 
\begin{lstlisting}
Class distributions
survival <= 11.5
0   1   
1.0 0.0 
survival > 11.5
0   1   
0.0 1.0 
survival is missing
0   1   
0.042105263157894736    0.9578947368421052
\end{lstlisting}
\paragraph{}Since there is no missing value for the survival attribute, this branch can be ignored.

\paragraph{}But allowing the machine learning algorithms to see the origin attributes from which we derived the prediction attribut leads to absurdity, since the actual prediction task is to say whether a patient will survive one year after suffering a heart attack or not \emph{depending on the medical characteristics}.


\subsection{Decision Tree on preprocessed data}

\paragraph{}The J48 Decision Tree shows a more useful tree on the preprocessed data:

\begin{lstlisting}
J48 pruned tree
epss <= 13100: 1 (90.63/1.95)
epss > 13100
|   fractional-shortening <= 190: 1 (2.25/0.05)
|   fractional-shortening > 190: 0 (2.11/0.11)
\end{lstlisting}

\paragraph{}The documentation of the data set explains the two attributes as follows:
\begin{quote}
``epss -- E-point septal separation, another measure of contractility. Larger numbers are increasingly abnormal.''
\end{quote}
\begin{quote}
`` fractional-shortening -- a measure of contracility around the heart lower numbers are increasingly abnormal''
\end{quote}

\paragraph{}Only the rule for the fractional shortening should be the other way round, because a low fractional shortening is a rather pathological sign. Although the decision tree has a high precision of almost 95\% it performs very imprecise, because none of the patients who died after one year are correctly classified:

\begin{table}[h]
\centering
\begin{tabular}{|l|ll|}
\hline
 & a &	b \\
\hline
a = 0  & 0 & 4 	\\
b = 1 & 1 & 90 	\\
\hline
\end{tabular}
\caption{Confusion matrix for Decision tree}
\end{table}

Using an unpruned tree doesn't improve the results:
\begin{lstlisting}
J48 unpruned tree


pericardial-effusion = 0
|   epss <= 13100: 1 (76.62/1.95)
|   epss > 13100
|   |   epss <= 15600: 0 (2.19/0.16)
|   |   epss > 15600: 1 (2.19/0.03)
pericardial-effusion = 1: 1 (14.0)
\end{lstlisting}
Here, the existence of a pericardial effusion (fluid around the heart) is assumed to be a healthy characteristic, but it is not. Accordingly, the precision of the J48 decreases from 95\% to 88\%.

\subsubsection{Random Forest}
Let's see if a random forest in place of a simple decision tree can make better result out of the echocardiogram data set. With default settings, i.e. default number of features \todo[inline]{calculate!} and 10 random trees with unlimited tree depth, the random forest shows us a rather disappointing result:
	
\begin{table}[h]
\centering
\begin{tabular}{|l|ll|}
\hline
 & a &	b \\
\hline
a=0 & 0 & 4 \\
b=1 & 2 & 91\\
\hline
\end{tabular}
\caption{Confusion matrix for Random Forest}
\end{table}

And the statistical measures:
\begin{lstlisting}
=== Detailed Accuracy By Class ===
                 TP Rate  FP Rate  Precision  Recall  F-Measure    Class
                 0        0.022    0          0       0            0
                 0.978    1        0.958      0.978   0.968        1
Weighted Avg.    0.938    0.96     0.918      0.938   0.928     
\end{lstlisting}
\paragraph{Parameter: Number of trees}
\todo[inline]{proper line break here}
Reducing the number of the trees continuously decreases the quality of the model, as shown in the accuracy details for 5 random trees:
\begin{lstlisting}
=== Detailed Accuracy By Class ===
                 TP Rate  FP Rate  Precision  Recall  F-Measure    Class
                 0        0.032    0          0       0            0  
                 0.968    1        0.957      0.968   0.963        1   
Weighted Avg.    0.928    0.96     0.918      0.928   0.923    
\end{lstlisting}
.. and for a single random tree:
\begin{lstlisting}
=== Detailed Accuracy By Class ===
                 TP Rate  FP Rate  Precision  Recall  F-Measure    Class
                 0        0.054    0          0       0            0  
                 0.946    1        0.957      0.946   0.951        1   
Weighted Avg.    0.907    0.961    0.917      0.907   0.912    
\end{lstlisting}
Reducing the number of the trees increases the number of false positives but doesn't catch any true positives.
\paragraph{Limiting the tree-depth}
Limitations in the depths of the random trees has also almost no effect on the accuracy of random forests on this data set. The only change to the outcome can be observed as soon the depth of the trees is limited to 6:

\begin{table}[h]
\centering
\begin{tabular}{|l|ll|}
\hline
 & a &	b \\
\hline
a=0 & 0 & 4 \\
b=1 & 1 & 92\\
\hline
\end{tabular}
\caption{Confusion matrix for Random Forest, limited depth}
\end{table}

Only one false positive has gone. Not really a remarkable change in the quality. This does not change if the tree-depth is further reduced.
\paragraph{Increasing the number of features}
Increasing the number of features chosen by each random tree seems to has also a low effect on the quality, because until a certain border of 5 chosen features the outcome is exactly the same as it was with default settings. But if we configure the random forests with 6 features we can spot our first true positive:

\begin{table}[h]
\centering
\begin{tabular}{|l|ll|}
\hline
 & a &	b \\
\hline
a=0 & 1 & 3 \\
b=1 & 2 & 91\\
\hline
\end{tabular}
\caption{Confusion matrix for Random Forest, increased number of features}
\end{table}

Which gives us the best achievable accuracy that we can reach with decision based algorithms:
\begin{lstlisting}
=== Detailed Accuracy By Class ===
                TP Rate  FP Rate  Precision  Recall  F-Measure   Class
                0.25     0.022    0.333      0.25    0.286       0
                0.978    0.75     0.968      0.978   0.973       1
Weighted Avg.   0.948    0.72     0.942      0.948   0.945      
\end{lstlisting}

\subsection{Support Vector Machine}

\paragraph{}We used the support vector machine implementation called `SMO' in WEKA. Since a support vector machine uses the numeric coordinates of the data records to do the classification we tried to normalize the data in advance to obtain better results. Since this normalization did not have any impact on the results (we tried this not only on this dataset) we followed that the support vector machine algorithm of the WEKA-toolkit automatically normalizes all the numeric values.

\paragraph{}The build of the model took not even a second since the dataset is that small. The same holds for the evaluation. We tried all the different kernel-functions for the classification but the result didn't change. The following table shows the results of the evaluation and if you take a close look on the results, you can see that all data records were classified to the same class which means that the SVM algorithm wasn't able to cope with the small number of data records which should be classified with value 0:

\begin{table}[h]
\centering
\begin{tabular}{|llllllll|}
	\hline
									&TP Rate   	&FP Rate   &Precision   &Recall  &F-Measure   &ROC Area  &Class\\
	\hline
									&0         	&0         &0         	&0       &  0         & 0.5      &0\\
  								&1         	&1         &0.959     	&1       &0.979       &	0.5      &1\\
  \hline
	Weighted Avg.		&0.959     	&0.959     &0.919   		&0.959   &0.939     	&0.5	&\\
	\hline
\end{tabular}
\caption{Results of the SVM-classification for the echocardiogram dataset}
\end{table}

\vspace{6pt}

\begin{table}[h]
\centering
\begin{tabular}{|l|ll|}
\hline
 & a &	b \\
\hline
a=0 & 0 & 4 \\
b=1 & 0 & 93\\
\hline
\end{tabular}
\caption{Confusion matrix for SVM}
\end{table}



\subsection{k-Nearest Neighbour}

As with the SVM-algorithm there was no normalization necessary (we tested this on different datasets). In WEKA there are different implementations of the kNN-algorithm available and we chose (for all our datasets) the brute force kNN-algorithm since this is the classic implementation of the kNN-algorithm where we were interested in the running time. We tried different parameters concerning the number of neighbours which are considered for classification. Changes on that parameter didn't make any differences. As with the SVM, the kNN-algorithm was not able to cope with the small number of data records which should be classified with value 0. The creation of the model doesn't take any time with a kNN-algorithm since there is no model which would be built: kNN is a lazy classification algorithm. The evaluation of the model took not even a second. In the following are the results of the classification. The results are exactly the same as for the SVM.

\begin{table}[h]
\centering
\begin{tabular}{|llllllll|}
	\hline
									&TP Rate   	&FP Rate   &Precision &Recall  &F-Measure   &ROC Area  &Class\\
	\hline
									&0         	&0         &0         &0       &  0         & 0.5      &0\\
  								&1         	&1         &0.959     &1       &0.979       &	0.5      &1\\
  \hline
	Weighted Avg.		&0.959     	&0.959     &0.919   	&0.959   &0.939     	&0.5				&\\
	\hline
\end{tabular}
\caption{Results of the kNN-classification for the echocardiogram dataset}
\end{table}

\vspace{6pt}

\begin{table}[h]
\centering
\begin{tabular}{|l|ll|}
\hline
 & a &	b \\
\hline
a=0 & 0 & 4 \\
b=1 & 0 & 93\\
\hline
\end{tabular}
\caption{Confusion matrix for kNN}
\end{table}


\subsection{Comparison}



% ------------------------------------
% A D U L T   D A T A    S E T
% ------------------------------------



\section{Adult data}

\paragraph{}The Adult or Census Income data sets contains 48842 instances of information about persons and the task is to predict if a person earns more than 50.000 dollar a year. The data set has 14 attributes and missing values.

\paragraph{}The instances origin from the 1994 Census Database and were extracted by Barry Becker. For having a clean data set, only records with age above 16 with minimium one working hour per week have been selected from the database. We performed a 10-fold-cross-validation on this dataset.

\subsection{Preprocessing}

The ``Adult'' dataset has missing values which we all replaced by the mean value of the respective attribute. We did no further preprocessing. As for the Echocardiogram dataset there was no normalization neccessary for the SVM or the kNN algorithms.


\subsection{Decision Tree}
Using a decision tree yields a precision about 86\%. Varying the parameters for this algorithm influences the quality of the results only slightly, as the following examples illustrate.

First of all, with the default settings of the J48 implementation of weka, i.e. using pruning with a confidence factor of 0.25 together with MDLCorrection and Subtree raising and at least 2 instances per leaf of the tree, we get the following summary of the quality:
 
\begin{lstlisting}
=== Detailed Accuracy By Class ===
                TP Rate  FP Rate  Precision  Recall  F-Measure  Class
                0.615    0.062    0.759      0.615   0.68       >50K
                0.938    0.385    0.885      0.938   0.911      <=50K
Weighted Avg.   0.86     0.307    0.854      0.86    0.855      
\end{lstlisting}

The decision tree that was used for this run has a size of 413 and has 311 leaves.

\paragraph{Raising the confidence factor}
Subsequently increasing the confidence factor, which result in less pruning, leads to a continuous worse quality. If we set the confidence factor to 0.5 we get the following outcome:
\begin{lstlisting}
=== Detailed Accuracy By Class ===
                TP Rate  FP Rate  Precision  Recall  F-Measure  Class
                0.632    0.071    0.738      0.632   0.681      >50K
                0.929    0.368    0.888      0.929   0.908      <=50K
Weighted Avg.   0.857    0.296    0.852      0.857   0.854      
\end{lstlisting}
.. for a confidence factor of 0.75 we get the following:
\begin{lstlisting}
=== Detailed Accuracy By Class ===
                TP Rate  FP Rate  Precision  Recall  F-Measure  Class
                0.655    0.094    0.689      0.655   0.672      >50K
                0.906    0.345    0.892      0.906   0.899      <=50K
Weighted Avg.   0.846    0.285    0.843      0.846   0.844     
\end{lstlisting}

\paragraph{Other parameters}
Varying other parameters has a similar effect like the one mentioned above. Either the quality of the J48 algorithm decreases a little or it stays the same, thus never exceeding an overall quality of 86\%. To sum it up: Using an unpruned tree yields the same quality as a tree with a confidence factor of 0.75. Smoothing the counts at the leaves with laplace has no effect. Either without MDLcorrection or without Subtree raising during pruning, in both cases the precision falls from 86\% to 85.5\%. If we rise the minimum of instances per leaves, e.g. to 20 instances and 50 instances, the precision falls from 86\% to 85\% and 84\$ respectively.

\subsection{Random Forest}

With default settings, a random forest achieves an accuracy as follows:
\begin{lstlisting}
=== Detailed Accuracy By Class ===
                TP Rate  FP Rate  Precision  Recall  F-Measure  Class
                0.615    0.108    0.644      0.615   0.629      >50K
                0.892    0.385    0.88       0.892   0.886      <=50K
Weighted Avg.   0.825    0.318    0.823      0.825   0.824     
\end{lstlisting}
which is surprisingly a worse result than from the J48 algorithm. 

\paragraph{Number of Random Trees}
As expected, reducing the number of random trees reduces the quality, with 5 random trees the precision lays at 82.6\% with only 3 random trees it lays at 82.3\%. Interestingly, the quality gets higher if we take even more than 10 random trees into account: With 15 trees, the precision is at 83.2\% and with 25 random trees it is at 83.3\%.

\paragraph{Limitations of the maximum depth of the trees}
We have a sign for overfitting under default settings: Reducing the maximum depth has a positive effect on the quality. From size 20 over 10 until the size of 7 the precision of the random forest rises until 85\%. Reducing the depth even further lets the precision fall again. Here is the detailed accuracy for a random forest which can grow only to a height of 6:

\begin{lstlisting}
=== Detailed Accuracy By Class ===
                TP Rate  FP Rate  Precision  Recall  F-Measure  Class
                0.606    0.064    0.749      0.606   0.67       >50K
                0.936    0.394    0.882      0.936   0.908      <=50K
Weighted Avg.   0.856    0.314    0.85       0.856   0.851     
\end{lstlisting}
We can push the accuracy even further, if we take more random trees, e.g. twenty:
\begin{lstlisting}
=== Detailed Accuracy By Class ===
                TP Rate  FP Rate  Precision  Recall  F-Measure  Class
                0.593    0.058    0.763      0.593   0.667      >50K
                0.942    0.407    0.879      0.942   0.909      <=50K
Weighted Avg.   0.858    0.323    0.851      0.858   0.851      
\end{lstlisting}

\paragraph{Number of features}
Increasing the number of features doesn't support the quality at all. The default number of features for this data set with 13 attributes is \todo[inline]{calculate!}. With a number of 5 randomly chosen features, the precision is yet at 85\% (with maximum depth of tree = 6) but with 10 randomly chosen features the precision falls back to 84.5\%.

\subsection{Support Vector Machine}

From the different kernel functions for the SVM-algorithm we used the PolyKernel on this dataset. As with the dataset before the normalization of the numeric data had no inpact on the validation results. One interesting aspect is that the creation of the model and the 10-fold-cross-validation took 135 minutes since the dataset is big. The SVM-algorithm was able to correctly classify 84,9\% of the instances. The following two tables show further validation results including a confusion matrix:

\begin{table}[h]
\centering
\begin{tabular}{|llllllll|}
	\hline
									&TP Rate   	&FP Rate   &Precision   &Recall  &F-Measure &ROC Area  	&Class\\
	\hline
									&0.568     	&0.062     &0.744     	&0.568   &0.644    	&0.753    	&	>50K\\
  								&0.938     	&0.432     &0.873     	&0.938   &0.904     &0.753    	&$\leq$50K\\
  \hline
	Weighted Avg.   & 0.849    & 0.343     &0.842   		&0.849   &0.842     & 0.753			&\\
	\hline
\end{tabular}
\caption{Results of the SVM-classification for the adult dataset}
\end{table}

\vspace{6pt}

\begin{table}[h]
\centering
\begin{tabular}{|l|ll|}
\hline
              &    a &     b \\
\hline
 a = >50k      & 4454 &  3387 \\
 b = $\leq$50k & 1531 & 23189\\
\hline
\end{tabular}
\caption{Confusion matrix for SVM}
\end{table}


\subsection{Naive Bayes}

We use 66\% of the data set for training and the rest for evaluation.

For the first run, we use the default setting and get the following result:

\begin{verbatim}
Correctly Classified Instances        9104               82.2329 %
Incorrectly Classified Instances      1967               17.7671 %
Total Number of Instances            11071     

Precision   Recall  F-Measure  Class
 0.592       0.786   0.675      >50K
 0.927       0.834   0.878      <=50K
\end{verbatim}

Second run: we use the discretizer with 20 bin.

\begin{verbatim}
Correctly Classified Instances        9287               83.8858 %
Incorrectly Classified Instances      1784               16.1142 %
Total Number of Instances            11071     

=== Detailed Accuracy By Class ===

Precision   Recall  F-Measure  Class
 0.621       0.805   0.701      >50K
 0.934       0.849   0.89       <=50K
\end{verbatim}

As expected, the result is slightly better.

A better result should be possible if we let the dicretizer split the values in part of nearly equal size. The result:

\begin{verbatim}
Correctly Classified Instances        9179               82.9103 %
Incorrectly Classified Instances      1892               17.0897 %
Total Number of Instances            11071     

Precision   Recall  F-Measure  Class
0.602        0.805   0.689      >50K
0.933        0.836   0.882      <=50K
\end{verbatim}

It got worse. \todo{why does discretizer with equal bin size not give a better result?}

The Naive Bayes does not have a lot of settings. It use a built in supervised discretizer or a kernel estimator. But the result is always the same with around 80\% correctly classified instances in the test run.

\subsection{k-Nearest Neighbour}

We tested the k-nearest neighbour algorithm with different values for $k$. For $k$=1 (nearest neighbour) the algorithm was able to correctly classify 79,4\% of the instances. We tested the algorithm also with values 10,40 and 100 for $k$ and obtained the best results with $k=40$, where 83,48\% of the instances were correctly classified. Again the normalization of the input had no influence on the results. The evaluation of this algorithm took 120 seconds. In the following tables we present further results of the evaluation with $k=40$ including a confusion matrix.

\begin{table}[h]
\centering
\begin{tabular}{|llllllll|}
	\hline
									&TP Rate   &FP Rate   &Precision   &Recall  &F-Measure &ROC Area  	&Class\\
	\hline
									&0.593     &0.088      &0.68      &0.593     &0.633    &0.884    		&>50K\\
									&0.912     &0.407      &0.876     &0.912     &0.893    &0.884    		&$\leq$50K\\
  \hline
	Weighted Avg.   &0.835     &0.331      &0.829     &0.835     &0.831    &0.884 \\
	\hline
\end{tabular}
\caption{Results of the kNN-classification for the adult dataset}
\end{table}

\vspace{6pt}

\begin{table}[h]
\centering
\begin{tabular}{|l|ll|}
	\hline
	 &              a    & b     \\
	\hline
    a = >50K      &	4647 & 3194  \\
    b = $\leq$50K & 2186 & 22534 \\
    \hline
\end{tabular}
\caption{Confusion matrix for kNN}
\end{table}


\subsection{Comparison}


% ------------------------------------
% C O N T R A S E P T I V E    S E T
% ------------------------------------


\section{Contraceptive Method Choice Data}

The Contraceptive Method Choice Data Set consists of information about the demographic and socio-economic characteristics of married women. There are 1473 instances with 9 different attributes. Examples of these attributes are the wife's age, husbands education, wife's religion, etc. The task is to predict which kind of contraceptive method a woman uses. These are categorized as no use, short term or long term. We performed a 10-fold-cross-validation on this dataset.

\subsection{Preprocessing}

The attributes are already in a numerical and categorical form, whereby the categories are encoded in an ordinal form. These go from 1 to 4 where 1 is low/bad and 4 is high/good.

\todo{check if ordinal is the correct term when i mean, that the attributes are encoded in numbers, where 1<2, 1<4, 4>2, ...}


\subsection{Desicion Tree}
\subsection{Random Forest}
\subsection{Naive Bayes}
\subsection{Support VectorMachine}

We tested different kernel functions on this dataset and obtained the best results with the NormalizedPolyKernel. The whole process of the evaluation, including the building of the models, took 28 seconds. Normalization of the data had again no influence on the results. The SVM algorithm was able to correctly classify 49,2\% of the instances. The following tables show further results of the evaluation with the NormalizedPolyKernel including a confusion matrix.

\begin{table}[h]
\centering
\begin{tabular}{|llllllll|}
	\hline
	&								TP Rate   	&FP Rate   &Precision   &Recall  &F-Measure   &ROC Area  	&Class\\
	\hline
									&0.583     	&0.355     &0.55      	&0.583   &0.566   		&0.628    	&1\\
                 	&0.393     	&0.181     &0.389     	&0.393   &0.391      	&0.648    	&2\\
                 	&0.444     	&0.252     &0.484     	&0.444   &0.463      	&0.614    	&3\\
  \hline
	Weighted Avg.   & 0.849    & 0.343      &0.842   &  0.849     &0.842     & 0.753			&\\
	\hline
\end{tabular}
\caption{Results of the SVM-classification for the contraception dataset}
\end{table}

\vspace{6pt}

\begin{table}[h]
\centering
\begin{tabular}{|l|lll|}
	\hline
	      &a   &b   &c \\
	\hline
	a = 1 &367 &105 &157 \\
 	b = 2 &117 &131 &85 \\
 	c = 3 &183 &101 &227 \\
  \hline
\end{tabular}
\caption{Confusion matrix for SVM}
\end{table}

\subsection{k-Nearest Neighbour}

We tested the kNN-algorithm with a lot of different values for $k$ and obtained the best results for $k=7$ where 48,88\% of the instances were correctly classified. The evaluation took not even a second. As with the other datasets the normalization of the input had no influence on the results. In the following tables we present further results of the evaluation with $k=7$ including a confusion matrix.

\begin{table}[h]
\centering
\begin{tabular}{|llllllll|}
	\hline
	&								TP Rate   	&FP Rate   &Precision   &Recall  &F-Measure   &ROC Area  	&Class\\
	\hline
									&0.569     	&0.351     &0.547     	&0.569   &0.558      	&0.65     	&1\\
                 	&0.414     	&0.166     &0.422     	&0.414   &0.418      	&0.677    	&2\\
                 	&0.438     	&0.279     &0.455     	&0.438   &0.447      	&0.603    	&3\\
  \hline
	Weighted Avg.   &0.489     	&0.284     &0.487     	&0.489   &0.488      	&0.64			&\\
	\hline
\end{tabular}
\caption{Results of the kNN-classification for the contraception dataset}
\end{table}

\vspace{6pt}

\begin{table}[h]
\centering
\begin{tabular}{|l|lll|}
	\hline
	      &a   &b   &c \\
	\hline
    a = 1 &358 &99  &172\\
    b = 2 &99  &138 &96 \\
 	c = 3 &197 &90  &224 \\
  \hline
\end{tabular}
\caption{Confusion matrix for kNN}
\end{table}

\subsection{Comparison}

\bibliography{references}


\end{document}
%------------------------------------------------


