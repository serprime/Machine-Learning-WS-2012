%x%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Short Sectioned Assignment
% LaTeX Template
% Version 1.0 (5/5/12)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Frits Wenneker (http://www.howtotex.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%   PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[paper=a4, fontsize=11pt]{scrartcl} % A4 paper and 11pt font size

\usepackage{color}
\usepackage{float}
\usepackage{placeins}
\usepackage{natbib}
% philipp: added for code listings
\usepackage{listings}
\lstset{
    captionpos=b
}
\newcommand{\Hilight}{\makebox[0pt][l]{\color{red}\rule[-4pt]{0.65\linewidth}{14pt}}}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{morefloats}
\bibliographystyle{plain}

\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
\usepackage{fourier} % Use the Adobe Utopia font for the document - comment this line to return to the LaTeX default
\usepackage[english]{babel} % English language/hyphenation
\usepackage{amsmath,amsfonts,amsthm} % Math packages

\usepackage{lipsum} % Used for inserting dummy 'Lorem ipsum' text into the template
\usepackage{todonotes}

\usepackage{sectsty} % Allows customizing section commands
\allsectionsfont{\centering \normalfont\scshape} % Make all sections centered, the default font and small caps

\usepackage{fancyhdr} % Custom headers and footers
\pagestyle{fancyplain} % Makes all pages in the document conform to the custom headers and footers
\fancyhead{} % No page header - if you want one, create it in the same way as the footers below
\fancyfoot[L]{} % Empty left footer
\fancyfoot[C]{} % Empty center footer
\fancyfoot[R]{\thepage} % Page numbering for right footer
% \renewcommand{\thesection}{Data Set \arabic{section}}
\renewcommand{\headrulewidth}{0pt} % Remove header underlines
\renewcommand{\footrulewidth}{0pt} % Remove footer underlines
\setlength{\headheight}{13.6pt} % Customize the height of the header

\numberwithin{equation}{section} % Number equations within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{figure}{section} % Number figures within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{table}{section} % Number tables within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)

\setlength\parindent{0pt} % Removes all indentation from paragraphs - comment this line for an assignment with lots of text

%----------------------------------------------------------------------------------------
%   TITLE SECTION
%----------------------------------------------------------------------------------------

\newcommand{\horrule}[1]{\rule{\linewidth}{#1}} % Create horizontal rule command with 1 argument of height

\title{ 
\normalfont \normalsize 
\textsc{Vienna University of Technology} \\ [25pt] % Your university, school and/or department name(s)
\horrule{0.5pt} \\[0.4cm] % Thin top horizontal rule
\huge Experiments in Machine Learning \\ % The assignment title
\horrule{2pt} \\[0.5cm] % Thick bottom horizontal rule
}

\author{Benjamin Kiesl \and Philipp Steinwender \and Robert Sch\"{a}fer} % Your name

\date{\normalsize\today} % Today's date or a custom date

\begin{document}

\maketitle % Print the title

%----------------------------------------------------------------------------------------
%   PROBLEM 1
%----------------------------------------------------------------------------------------

\tableofcontents

\section{Preface}

\paragraph{}For the first exercise 3 data sets were chosen on which five classification algorithms were tested to practice basic machine learning tasks and see how these algorithms behave with their various settings. For the elaboration of this paper the Machine Learning Software Weka was used. It offered all necessary algorithms.

\paragraph{}As data sets we chose as follows:

\begin{itemize}
\item Echo Cardiogram, a data set about heart attacks and if a patient survived one year after the attack.
\item Adult / Census Income, a data set about people and their basic information and if they earn more than 50k a year.
\item Contraceptive Method Choice, a data set about women and their environment and basic information and what contraceptive method they use.
\end{itemize}

\paragraph{}Five classification algorithms were chosen that we used on all data sets. Each chapter will also dicuss how the classifiers perform an the data and why.


% ------------------------------------
% E C H O   C A R D I O    S E T
% ------------------------------------


\section{Echo Cardiogram data}

\paragraph{}The Echocardiogram data set originally consists of 132 entries with 13 attributes respectively. The entries of the dataset refer to patients that suffered from a heart attack at some point in the past \cite{uci-repo}. The prediction task is to determine from several different attributes if the patient will survive at least one year, beginning from the day when the heart attack occured. The dataset contains missing values and there's a lot of other preprocessing steps (described later) which have to be done. The dataset is very small, especially after preprocessing and a further problem is that the number of patients which died within one year after their heart attack is very small (4) which makes the classification very hard. It is nevertheless an interesting question wether some classification algorithm can perform well on that data. Since it is very small we performed a 3-fold-cross-validation on this dataset.


\subsection{Preprocessing}

\paragraph{}The patient's attribute ``name'' was replaced by a dummy for all entries. For this reason, this attribute could be entirely removed. Other unnecessary attributes were ``group'' and ``mult'' which were also (as explained in the description) just meaningless and could thus be ignored \cite{uci-repo}. There were also two attributes ``wall-motion-score'' and ``wall-motion-index'' where the description of the dataset said that one should only use the ``wall-motion-index'' for classification issues and so ``wall-motion-score'' was also removed.

\paragraph{}The actual prediction attribute ``alive-at-1'' (denoting that a patient lived at least one year after having a heart attack) had to be inferred from two other attributes, namely ``survival'' (number of months that the test-person survived after the heart attack) and ``still-alive'' (a binary variable denoting if a patient is still alive at the time when the data was recorded). It is possible that the ``survival'' variable has a value lower than 12 but the ``still-alive'' variable is set to true, meaning that the test-person still lived. This is the case if the data for a test-person was recorded in the last 12 months and so this data cannot be used for the classification and was thus removed.

\paragraph{}It should be clear that the variables ``survival'' and ``still-alive'' should not be used for classification since they constitute the final prediction attribute. Listing \ref{lst:silly-tree} shows what happens if a decision tree can take these attributes into account. It states that if the patient survived $ <= 12 $ months, he survived at least 12 month. Pretty accurate, right? 
The only remarkable thing is, that the J48 implementation under default settings seems to enforce a certain depth of the tree, even if the splitting on the same attribute is redundant since $ x > 11 \rightarrow x > 12 $. 

\begin{lstlisting}[caption={Decision tree on 100\% dependant attributes}, label=lst:silly-tree, float=htb]
J48 pruned tree
------------------
    survival <= 12
    |   survival <= 11: 0 (4.0)
    |   survival > 11: 1 (4.0)
    survival > 12: 1 (87.0)
\end{lstlisting}

\paragraph{}The last preprocessing step was the replacement of missing values (data imputation). Most of the data was complete but there were still some data records having missing values which we replaced by the mean values of the specific variables. One data record did contain completeley useless data and was thus removed (see in listing \ref{lst:crappy-entry}).

\begin{lstlisting}[escapechar=\%, float=htb, caption={76 years old patient without useful information}, label=lst:crappy-entry]
20,1,59,0,0.030,21.300,6.290,17,1.310,0.928,name,2,0
0.25,1,63,1,?,?,?,23,2.300,0.714,name,2,1
%\Hilight% ?,?,?,77,?,?,?,?,?,2,?,name,2,?
2,1,56,1,0.040,14,5,?,?,?,name,2,1
7,1,61,1,0.270,?,?,9,1.500,0.428,name,2,1
\end{lstlisting}

\paragraph{}

%So long, these preprocessing tasks are just removing attributes (column-wise) or removing certain entries (row-wise), depending on their values.
%
%Unfortunately, there is another peculiarity with the actual prediction attribute ``alive-at-1'', here is the official description:
%\begin{quotation} ``Boolean-valued. Derived from the first two attributes. 0 means patient was either dead after 1 year or had been followed for less than 1 year.  1 means patient was alive at 1 year.''\cite{uci-repo} \end{quotation}
%De facto, the values don't fit to this description. The attribute is $1$ if the patient was followed less than one year and $0$ otherwise. Very astonishing: Even if the attribute is supposed to be a derived attribute, there are several \texttt{null} values, making the corresponding values useless for training data.                                                                
%
%We thus inferred the prediction attribute ``alive-at-1'' as explained in the description of the whole dataset: For all patients where the months of survival are greater than or equal to 12 we set the ``alive-at-1''-attribute to 1, otherwise we set it to 0.

%The only thing which remained then was to a

\subsection{Decision Tree}

With default settings J48 Decision Tree seems to show a reasonable tree to some extent as seen in listing \ref{lst:echo:dec:default}.

\begin{lstlisting}[label=lst:echo:dec:default, caption={Vizualization of Decision Tree on echocardiogram data -- Default Settings}]
J48 pruned tree
------------------
epss <= 13100: 1 (90.63/1.95)
epss > 13100
|   fractional-shortening <= 190: 1 (2.25/0.05)
|   fractional-shortening > 190: 0 (2.11/0.11)
\end{lstlisting}

\paragraph{}The documentation of the data set explains the two attributes as follows:

\begin{quote}
``epss -- E-point septal separation, another measure of contractility. Larger numbers are increasingly abnormal.''
\end{quote}

\begin{quote}
`` fractional-shortening -- a measure of contracility around the heart lower numbers are increasingly abnormal''
\end{quote}

\paragraph{}Only the rule for the fractional shortening should be the other way round, because a low fractional shortening is a rather pathological sign. Although the decision tree has a high precision of almost 95\% it performs very imprecise, because none of the patients who died after one year are correctly classified:

\begin{table*}[htb]\centering
    \begin{tabular}{@{}ll|ll@{}} 
\multicolumn{2}{c}{\phantom{bla}}  & \multicolumn{2}{c}{Classified as} \\  
                        & \phantom{aa}  &  0  &  1  \\ \cmidrule{2-4}
\multirow{2}{*}{Instances}     &  0  &  0  &  4  \\
                                   &  1  &  1  &  90 \\ 
\end{tabular}
\caption{Confusion matrix for Decision tree}
\end{table*}

Using an unpruned tree doesn't improve the results:
\begin{lstlisting}
J48 unpruned tree
pericardial-effusion = 0
|   epss <= 13100: 1 (76.62/1.95)
|   epss > 13100
|   |   epss <= 15600: 0 (2.19/0.16)
|   |   epss > 15600: 1 (2.19/0.03)
pericardial-effusion = 1: 1 (14.0)
\end{lstlisting}
Here, the decision tree assumes a pericardial effusion (fluid around the heart) to be a healthy characteristic, but it is not. Accordingly, the precision of the J48 decreases from 95\% to 88\%. Using a higher confidence factor instead of a totally unpruned tree has no effect on the quality or the classification distribution. 

\paragraph{Other parameter}
Neither the laplace correctin to smooth the count of the leaves nor reduced error pruning has any effect on the quality of result. Without mdl correction the precision decreases a little to 91.9\%. Raising the minimum of instances per leaf has no effect either, which is no suprise since the data set is rather small. We couldn't get a better result with the J48 algorithm than to classify all patients to survive at least one year. The detailed accuracy for this simple, single-sided classification can be seen in table
\ref{tab:echo:dec:best}.  


\begin{table*}[htb]\centering
    \begin{tabular*}{\columnwidth}{@{}lllllll@{}}
        \toprule 
               &  TP Rate & FP Rate & Precision & Recall & F-Measure &  Class  \\ \midrule
               &  0       & 0       & 0         & 0      & 0         &  0      \\     
               &  1       & 1       & 0.959     & 1      & 0.979     &  1      \\     
Weighted Avg.  &  0.959   & 0.959   & 0.919     & 0.959  & 0.939     &         \\ \bottomrule
    \end{tabular*}
\caption{Decision Tree -- best achievable results} 
\label{tab:echo:dec:best}
\end{table*}
\FloatBarrier



\subsubsection{Random Forest}
Let's see if a random forest in place of a simple decision tree can make better result out of the echocardiogram data set. With default settings, i.e. default number of features $ log_2(7) + 1 \approx 4 $, and 10 random trees with unlimited tree depth, the random forest shows us a rather disappointing result as shown in the confusion matrix in table \ref{tab:echo:rand:matrix1} and the accuracy details in table \ref{tab:echo:rand:default}:
	

\begin{table*}[htb]\centering
    \begin{tabular}{@{}ll|ll@{}} 
\multicolumn{2}{c}{\phantom{bla}}  & \multicolumn{2}{c}{Classified as} \\  
                   & \phantom{aa}        &  0  &  1  \\ \cmidrule{2-4}
\multirow{2}{*}{Instances}         &  0  &  0  &  4  \\
                                   &  1  &  2  &  91 \\ 
\end{tabular}
\caption{Confusion matrix for Random Forest}
\label{tab:echo:rand:matrix1}
\end{table*}

\begin{table*}[htb]\centering
    \begin{tabular*}{\columnwidth}{@{}lllllll@{}}
        \toprule 
              &  TP Rate & FP Rate & Precision & Recall & F-Measure & Class \\ \midrule
              &  0       & 0.022   & 0         & 0      & 0         & 0     \\
              &  0.978   & 1       & 0.958     & 0.978  & 0.968     & 1     \\
Weighted Avg. &  0.938   & 0.96    & 0.918     & 0.938  & 0.928     &       \\ \bottomrule
    \end{tabular*}
\caption{Random Forest -- Default Settings} 
\label{tab:echo:rand:default}
\end{table*}
\FloatBarrier


\paragraph{Number of Random Trees}
Reducing the number of the trees continuously decreases the quality of the model, tables \ref{tab:echo:rand:5t} and \ref{tab:echo:rand:1t} show the regression of quality.
This method increases the number of false positives but doesn't catch any true positives.
\begin{table*}[htb]\centering
    \begin{tabular*}{\columnwidth}{@{}lllllll@{}}
        \toprule 
               &  TP Rate & FP Rate & Precision & Recall & F-Measure &   Class  \\ \midrule
               &  0       & 0.032   & 0         & 0      & 0         &   0      \\
               &  0.968   & 1       & 0.957     & 0.968  & 0.963     &   1      \\     
Weighted Avg.  &  0.928   & 0.96    & 0.918     & 0.928  & 0.923     &          \\ \bottomrule
    \end{tabular*}
\caption{Random Forest -- 5 Random Trees} 
\label{tab:echo:rand:5t}
\end{table*}
\FloatBarrier

\begin{table*}[htb]\centering
    \begin{tabular*}{\columnwidth}{@{}lllllll@{}}
        \toprule 
               &  TP Rate & FP Rate & Precision & Recall & F-Measure &   Class \\ \midrule
               &  0       & 0.054   & 0         & 0      & 0         &   0     \\ 
               &  0.946   & 1       & 0.957     & 0.946  & 0.951     &   1     \\ 
Weighted Avg.  &  0.907   & 0.961   & 0.917     & 0.907  & 0.912     &         \\ \bottomrule
    \end{tabular*}
\caption{Random Forest -- 1 Random Tree} 
\label{tab:echo:rand:1t}
\end{table*}
\FloatBarrier
\paragraph{Limiting the maximum depth of the trees}
Limitations in the depths of the random trees has also almost no effect on the accuracy of random forests on this data set. The only change to the outcome can be observed as soon the depth of the trees is limited to 6 (see table \ref{tab:echo:rand:matrix2}).
Only one false positive has gone. Not really a remarkable change in the quality. This does not change if the tree-depth is further reduced.
\begin{table*}[htb]\centering
    \begin{tabular}{@{}ll|ll@{}} 
\multicolumn{2}{c}{\phantom{bla}}  & \multicolumn{2}{c}{Classified as} \\  
                   & \phantom{aa}        &  0  &  1  \\ \cmidrule{2-4}
\multirow{2}{*}{Instances}         &  0  & 0 & 4 \\
                                   &  1  & 1 & 92\\ 
\end{tabular}
\caption{Confusion matrix for Random Forest, limited depth}
\label{tab:echo:rand:matrix2}
\end{table*}



\paragraph{Increasing the number of features}
Increasing the number of features chosen by each random tree seems to has also a low effect on the quality, because until a certain border of 5 chosen features the outcome is exactly the same as it was with default settings. But if we configure the random forests with 6 features we can spot our first true positive in the confusion matrix (table \ref{tab:echo:rand:matrix3}).
This gives us the best achievable accuracy that we can reach with decision based algorithms, shown in table \ref{tab:echo:rand:6f}.


\begin{table*}[htb]\centering
    \begin{tabular}{@{}ll|ll@{}} 
\multicolumn{2}{c}{\phantom{bla}}  & \multicolumn{2}{c}{Classified as} \\  
                   & \phantom{aa}        &  0  &  1  \\ \cmidrule{2-4}
\multirow{2}{*}{Instances}         &  0  & 1 & 3 \\
                                   &  1  & 2 & 91\\ 
\end{tabular}
\caption{Confusion matrix for Random Forest, increased number of features}
\label{tab:echo:rand:matrix3}
\end{table*}

\begin{table*}[htb]\centering
    \begin{tabular*}{\columnwidth}{@{}lllllll@{}}
        \toprule 
               &  TP Rate &  FP Rate &  Precision &  Recall &  F-Measure &  Class \\ \midrule
               &  0.25    &  0.022   &  0.333     &  0.25   &  0.286     &  0     \\
               &  0.978   &  0.75    &  0.968     &  0.978  &  0.973     &  1     \\
Weighted Avg.  &  0.948   &  0.72    &  0.942     &  0.948  &  0.945     &        \\ \bottomrule
    \end{tabular*}
\caption{Random Forest -- 6 randomly chosen features} 
\label{tab:echo:rand:6f}
\end{table*}
\FloatBarrier

\subsection{Support Vector Machine}

\paragraph{}We used the support vector machine implementation called `SMO' in WEKA. Since a support vector machine uses the numeric coordinates of the data records to do the classification we tried to normalize the data in advance to obtain better results. Since this normalization did not have any impact on the results (we tried this not only on this dataset) we followed that the support vector machine algorithm of the WEKA-toolkit automatically normalizes all the numeric values.

\paragraph{}The build of the model took not even a second since the dataset is that small. The same holds for the evaluation. We tried all the different kernel-functions for the classification but the result didn't change. The following table shows the results of the evaluation and if you take a close look on the results, you can see that all data records were classified to the same class which means that the SVM algorithm wasn't able to cope with the small number of data records which should be classified with value 0:

\begin{table}[h]
\centering
\begin{tabular}{lllllll}
	\toprule
									&TP Rate   	&FP Rate   &Precision   &Recall  &F-Measure   &Class\\
	\midrule
									&0         	&0         &0         	&0       &  0              &0\\
  								&1         	&1         &0.959     	&1       &0.979             &1\\
  
	Weighted Avg.		&0.959     	&0.959     &0.919   		&0.959   &0.939     		&\\
	\bottomrule
\end{tabular}
\caption{Results of the SVM-classification for the echocardiogram dataset}
\end{table}

\vspace{6pt}

\begin{table}[h]
\centering
\begin{tabular}{|l|ll|}
\hline
 & a &	b \\
\hline
a=0 & 0 & 4 \\
b=1 & 0 & 93\\
\hline
\end{tabular}
\caption{Confusion matrix for SVM}
\end{table}


\subsection{k-Nearest Neighbour}

\paragraph{}As with the SVM-algorithm there was no normalization necessary (we tested this on different datasets). In WEKA there are different implementations of the kNN-algorithm available and we chose (for all our datasets) the brute force kNN-algorithm since this is the classic implementation of the kNN-algorithm where we were interested in the running time. We tried different parameters concerning the number of neighbours which are considered for classification. Changes on that parameter didn't make any differences. As with the SVM, the kNN-algorithm was not able to cope with the small number of data records which should be classified with value 0. The creation of the model doesn't take any time with a kNN-algorithm since there is no model which would be built: kNN is a lazy classification algorithm. The evaluation of the model took not even a second. In the following are the results of the classification. The results are exactly the same as for the SVM.

\begin{table}[h]
\centering
\begin{tabular}{lllllll}
	\toprule
									&TP Rate   	&FP Rate   &Precision &Recall  &F-Measure   &Class\\
	\midrule
									&0         	&0         &0         &0       &  0         &0\\
  								&1         	&1         &0.959     &1       &0.979       &1\\
  
	Weighted Avg.		&0.959     	&0.959     &0.919   	&0.959   &0.939     					&\\
	\bottomrule
\end{tabular}
\caption{Results of the kNN-classification for the echocardiogram dataset}
\end{table}

\vspace{6pt}

\begin{table}[h]
\centering
\begin{tabular}{|l|ll|}
\hline
 & a &	b \\
\hline
a=0 & 0 & 4 \\
b=1 & 0 & 93\\
\hline
\end{tabular}
\caption{Confusion matrix for kNN}
\end{table}


\subsection{Naive Bayes}

\paragraph{}For the fist try, the unsupervised discretizer was used. But the result was bad. The classifier could predict one single instance for for patients that did not survive one year.

\paragraph{}For the second try we prepare the data set with the PKIDiscretizer. The description says it "Discretizes numeric attributes using equal frequency binning, where the number of bins is equal to the square root of the number of non-missing values." But as we can see in \ref{tab:echo:bayes:1c} this discretizer does not perform better.

\begin{table}[h]
\centering
\begin{tabular}{|l|ll|}
\hline
 & a &	b \\
\hline
a=0 & 0 & 4 \\
b=1 & 1 & 92\\
\hline
\end{tabular}
\caption{Confusion matrix for kNN}
\label{tab:echo:bayes:1c}
\end{table}

\paragraph{}The log output of Naive Bayes shows that each bin has 1-2 negative (alive-at-1 = 0) instances. This means that each bin has the same low possibility for TP-prediction.

\paragraph{}So we have to boost the instances we want to predict. For that the SMOTE filter is used. It produces more instances of a given class index by looking at near neighbours with the same class value. The filter was applied several times, until the number of instances that have alive-at-1 = 0 1.2 times more than the alive-at-1 = 1 instances (128:93). The SMOTE filter used 3 nearest neighbours and produced 100\% new instances in each run. In this way we raised the numbers of instances with an outcome we want to predict, as for this training set it is better to predict if a patient is at risk to not survive one year.

\paragraph{}We have to supply a separate set for validation as we kind of destroy the data set when generating new instances. For that the original training set was taken. Although it is not good to validate a classifier with the same set as the training set, we can use this approach because our training set does not have a lot in common with the original set as we added a lot of generated instances.


\paragraph{}The confusion matrix \ref{tab:echo:bayes:2c} shows, that we predicted three out of four correct instances and also had more errors when predicting alive-at-1 = 0 than  alive-at-1 = 1.

\begin{table}[h]
\centering
\begin{tabular}{|l|ll|}
\hline
 & a &	b \\
\hline
a=0 & 3 & 1 \\
b=1 & 9 & 84\\
\hline
\end{tabular}
\caption{Confusion matrix for kNN}
\label{tab:echo:bayes:2c}
\end{table}

\paragraph{}Also the detailed accuracy table \ref{tab:echo:bayes:2d} shows that this approach gained a high recall with a low precision for alive-at-1 = 0 and both high precision and recall for alive-at-1 = 1.

\begin{table*}[htb]\centering
  \begin{tabular*}{\columnwidth}{@{}lllllll@{}}
      \toprule 
              &  TP Rate & FP Rate & Precision & Recall & F-Measure & Class    \\  \midrule
              &   0.75   & 0.097   & 0.25      & 0.75   & 0.375     & 0 \\
              &   0.903  & 0.25    & 0.988     & 0.903  & 0.944     & 1 \\
Weighted Avg. &   0.897  & 0.244   & 0.958     & 0.897  & 0.92      &   \\ \bottomrule
  \end{tabular*}
\caption{Naive Bayes with SMOTE filter} 
\label{tab:echo:bayes:2d}
\end{table*}


\subsection{Comparison}

The table below shos a comparison of the different algorithms on the adult dataset. For an algorithm we present here only the best obtained result concerning the TP rate. Values are always the weighted average.

\begin{table}[h]
\centering
\begin{tabular}{llllllll}
	\toprule
									&TP Rate   	&FP Rate   &Precision &Recall  &F-Measure   &  Correctly classified\\
	\midrule
	Decision Tree		\\
  Random Forest		\\
  Naive Bayes			&   0.897  & 0.244   & 0.958     & 0.897  & 0.92     \\
  SVM							&0.959     	&0.959     	&0.919   	&0.959   &0.939     	&	0.959\\
  kNN							&0.959     	&0.959     	&0.919   	&0.959   &0.939     	&	0.959\\
	\bottomrule
\end{tabular}
\caption{Comparison of the different algorithms for the echocardiogram dataset}
\end{table}

%% 
%%
\begin{table*}[htb]\centering
  \begin{tabular*}{\columnwidth}{@{}lllllll@{}}
      \toprule 
              &  TP Rate & FP Rate & Precision & Recall & F-Measure & Class    \\  \midrule
              &   0.75   & 0.097   & 0.25      & 0.75   & 0.375     & 0 \\
              &   0.903  & 0.25    & 0.988     & 0.903  & 0.944     & 1 \\
Weighted Avg. &   0.897  & 0.244   & 0.958     & 0.897  & 0.92      &   \\ \bottomrule
  \end{tabular*}
\caption{Echocardiogram -- Naive Bayes with SMOTE filter} 
\label{tab:echo:bayes:2d}
\end{table*}

\todo{todo marker}


% ------------------------------------
% A D U L T   D A T A    S E T
% ------------------------------------



\section{Adult data}

\paragraph{}The Adult or Census Income data sets contains 48842 instances of information about persons and the task is to predict if a person earns more than 50.000 dollar a year. The data set has 14 attributes and missing values.

\paragraph{}The instances origin from the 1994 Census Database and were extracted by Barry Becker. For having a clean data set, only records with age above 16 with minimium one working hour per week have been selected from the database. We performed a 10-fold-cross-validation on this dataset.

\subsection{Preprocessing}

\paragraph{}The ``Adult'' dataset has missing values which we all replaced by the mean value of the respective attribute. We did no further preprocessing. As for the Echocardiogram dataset there was no normalization neccessary for the SVM or the kNN algorithms.


\subsection{Decision Tree}
\paragraph{}Using a decision tree yields a precision about 86\%. Varying the parameters for this algorithm influences the quality of the results only slightly, as the following examples illustrate.

\paragraph{}First of all, with the default settings of the J48 implementation of weka, i.e. using pruning with a confidence factor of 0.25 together with mdl correction and subtree raising and at least 2 instances per leaf of the tree, we get a summary of the quality as shown in table \ref{tab:adult:dec:default}:
The decision tree that was used for this run has a size of 413 and has 311 leaves.

\begin{table*}[htb]\centering
    \begin{tabular*}{\columnwidth}{@{}lllllll@{}}
        \toprule 
               & TP Rate & FP Rate & Precision & Recall &  F-Measure &  Class \\ \midrule  
               & 0.615   & 0.062   & 0.759     & 0.615  &  0.68      &  >50K  \\  
               & 0.938   & 0.385   & 0.885     & 0.938  &  0.911     &  <=50K \\  
Weighted Avg.  & 0.86    & 0.307   & 0.854     & 0.86   &  0.855     &        \\  \bottomrule 
    \end{tabular*}
\caption{Decision Tree -- Default Settings} 
\label{tab:adult:dec:default}
\end{table*}
\FloatBarrier


\paragraph{Raising the confidence factor}
Subsequently increasing the confidence factor, which result in less pruning, leads to a continuous worse quality. If we set the confidence factor to 0.5 we get an outcome as shown in table \ref{tab:adult:dec:cf5} and if we set it to 0.75 the quality gets even worse as seen in \ref{tab:adult:dec:cf75}.
\begin{table*}[htb]\centering
    \begin{tabular*}{\columnwidth}{@{}lllllll@{}}
        \toprule 
              &  TP Rate & FP Rate & Precision & Recall & F-Measure & Class    \\  \midrule
              &  0.632   & 0.071   & 0.738     & 0.632  & 0.681     & >50K     \\ 
              &  0.929   & 0.368   & 0.888     & 0.929  & 0.908     & <=50K    \\ 
Weighted Avg. &  0.857   & 0.296   & 0.852     & 0.857  & 0.854     &          \\  \bottomrule
    \end{tabular*}
\caption{Decision Tree -- Confidence Factor of 0.5} 
\label{tab:adult:dec:cf5}
\end{table*}
\FloatBarrier


\begin{table*}[htb]\centering
    \begin{tabular*}{\columnwidth}{@{}lllllll@{}}
        \toprule 
              &  TP Rate & FP Rate & Precision & Recall &  F-Measure &  Class  \\ \midrule        
              &  0.655   & 0.094   & 0.689     & 0.655  &  0.672     &  >50K   \\          
              &  0.906   & 0.345   & 0.892     & 0.906  &  0.899     &  <=50K  \\          
Weighted Avg. &  0.846   & 0.285   & 0.843     & 0.846  &  0.844     &         \\ \bottomrule          
    \end{tabular*}
\caption{Decision Tree -- Confidence Factor of 0.75} 
\label{tab:adult:dec:cf75}
\end{table*}
\FloatBarrier

\paragraph{Other parameters}
Varying other parameters has a similar effect like the one mentioned above. Either the quality of the J48 algorithm decreases a little or it stays the same, thus never exceeding an overall quality of 86\%. To sum it up: Using an unpruned tree yields the same quality as a tree with a confidence factor of 0.75. Smoothing the counts at the leaves with laplace has no effect. Either without mdl correction or without Subtree raising during pruning, in both cases the precision falls from 86\% to 85.5\%. If we rise the minimum of instances per leaves, e.g. to 20 instances and 50 instances, the precision falls from 86\% to 85\% and 84\% respectively.



\subsection{Random Forest}

With default settings, a random forest gains an accuracy on the census income data set that can be seen in table \ref{tab:adult:rand:default}, which is surprisingly worse than the J48 algorithm.
\begin{table*}[htb]\centering
  \begin{tabular*}{\columnwidth}{@{}lllllll@{}}
      \toprule 
              &  TP Rate & FP Rate & Precision & Recall & F-Measure & Class    \\  \midrule
              &  0.615   & 0.108   & 0.644     & 0.615  & 0.629     & >50K     \\     
              &  0.892   & 0.385   & 0.88      & 0.892  & 0.886     & <=50K    \\ 
Weighted Avg. &  0.825   & 0.318   & 0.823     & 0.825  & 0.824     &          \\ \bottomrule
    \end{tabular*}
\caption{Random Forest -- Default Settings} 
\label{tab:adult:rand:default}
\end{table*}
\FloatBarrier


\paragraph{Number of Random Trees}
As expected, reducing the number of random trees reduces the quality, with 5 random trees the precision lays at 82.6\% with only 3 random trees it lays at 82.3\%. Interestingly, the quality gets higher if we take even more than 10 random trees into account: With 15 trees, the precision is at 83.2\% and with 25 random trees it is at 83.3\%.

\paragraph{Limiting the maximum depth of the trees}
We have a sign for overfitting under default settings: Reducing the maximum depth has a positive effect on the quality. From size 20 over 10 until the size of 7 the precision of the randomeforest rises until 85\%. Reducing the depth even further lets the precision fall again. Table \ref{tab:adult:rand:6d} shows the detailed accuracy for a random forest of seven trees which can grow only to a height of 6:

\begin{table*}[htb]\centering
  \begin{tabular*}{\columnwidth}{@{}lllllll@{}}
      \toprule 
               &  TP Rate & FP Rate & Precision & Recall & F-Measure & Class \\ \midrule
               &  0.591   & 0.057   & 0.766     & 0.591  & 0.667     & >50K  \\
               &  0.943   & 0.409   & 0.879     & 0.943  & 0.91      & <=50K \\
Weighted Avg.  &  0.858   & 0.324   & 0.852     & 0.858  & 0.851     &       \\ \bottomrule
    \end{tabular*}
\caption{Random Forest -- 7 Random Trees each not more than 6 branches high} 
\label{tab:adult:rand:6d}
\end{table*}
\FloatBarrier

\paragraph{Number of features}
Increasing the number of features doesn't support the quality at all. The default number of features for this data set with 13 attributes is $ log_2(13) + 1 \approx 5 $. Choosing 6 attributes brinchoose gs us a precision of 84,9\% and with 10 randomly chosen features the precision falls back to 84.5\%. We can tease the most out of the random forest if we decrease the number of features a little. To be concise, we get the best achievable result, if we take only 3 randomly chosen
features. Table \ref{tab:adult:rand:3f} shows these results which are of similar accuracy like the best of the J48 algorithm. 

\begin{table*}[htb]\centering
  \begin{tabular*}{\columnwidth}{@{}lllllll@{}}
      \toprule 
               &  TP Rate & FP Rate & Precision & Recall & F-Measure & Class \\   \midrule
               &  0.577   & 0.052   & 0.779     & 0.577  & 0.663     & >50K  \\   
               &  0.948   & 0.423   & 0.876     & 0.948  & 0.911     & $\leq$50K \\   
Weighted Avg.  &  0.859   & 0.333   & 0.853     & 0.859  & 0.851     &       \\   \bottomrule
    \end{tabular*}
\caption{Random Forest -- 3 Random Features} 
\label{tab:adult:rand:3f}
\end{table*}
\FloatBarrier


\subsection{Support Vector Machine}

\paragraph{}From the different kernel functions for the SVM-algorithm we used the PolyKernel on this dataset. As with the dataset before the normalization of the numeric data had no inpact on the validation results. One interesting aspect is that the creation of the model and the 10-fold-cross-validation took 135 minutes since the dataset is big. The SVM-algorithm was able to correctly classify 84,9\% of the instances. The following two tables show further validation results including a confusion matrix:

\begin{table}[h]
\centering
\begin{tabular}{llllllll}
	\toprule
									&TP Rate   	&FP Rate   &Precision   &Recall  &F-Measure  	&Class\\
	\midrule
									&0.568     	&0.062     &0.744     	&0.568   &0.644    	    	&	>50K\\
  								&0.938     	&0.432     &0.873     	&0.938   &0.904         	&$\leq$50K\\
  
	Weighted Avg.   & 0.849    & 0.343     &0.842   		&0.849   &0.842     			&\\
	\bottomrule
\end{tabular}
\caption{Results of the SVM-classification for the adult dataset}
\end{table}

\vspace{6pt}

\begin{table}[h]
\centering
\begin{tabular}{|l|ll|}
\hline
              &    a &     b \\
\hline
 a = >50k      & 4454 &  3387 \\
 b = $\leq$50k & 1531 & 23189\\
\hline
\end{tabular}
\caption{Confusion matrix for SVM}
\end{table}


\subsection{Naive Bayes}

\paragraph{}As further preprocessing for Naive Bayes it is important to dicretize the numerical attributes. The PKIDescretizer was used again as it produces better bin sizes than the standard discretizer without any configuration. After the first run with the Simple Naive Bayes algorithm we gain the result shown in table \ref{tab:adult:bayes:1d} and a confusion matrix \ref{tab:adult:bayes:1c}.


\begin{table*}[htb]\centering
  \begin{tabular*}{\columnwidth}{@{}lllllll@{}}
      \toprule 
               &  TP Rate & FP Rate & Precision & Recall & F-Measure & Class \\   \midrule
               &  0.806   & 0.163   & 0.611     & 0.806  & 0.695     & >50K  \\
               &  0.837   & 0.194   & 0.932     & 0.837  & 0.882     & <=50K \\
Weighted Avg.  &  0.83    & 0.186   & 0.854     & 0.83   & 0.837     &       \\   \bottomrule
    \end{tabular*}
\caption{Naive Bayes -- preprocessed with PKIDiscretizer} 
\label{tab:adult:bayes:1d}
\end{table*}



\begin{table}[h]
\centering
\begin{tabular}{|l|ll|}
\hline
              &    a &     b \\
\hline
 a = >50k      & 6321 &1520   \\
 b = $\leq$50k & 4025 &20695 \\
\hline
\end{tabular}
\caption{Confusion matrix for Naive Bayes}
\label{tab:adult:bayes:1c}
\end{table}

\paragraph{}By using different discretizers and also do not seperately discretize in the preprocessing section of weka but instead let the NaiveBayers classifier use it's own discretizers namely KernelEstimator or SupervisedDiscretizer the outcome of precision and recall differ a bit by 0.05 to 0.1 higher and lower. It does obviously do not really make a difference.



\subsection{k-Nearest Neighbour}

\paragraph{}We tested the k-nearest neighbour algorithm with different values for $k$. For $k$=1 (nearest neighbour) the algorithm was able to correctly classify 79,4\% of the instances. We tested the algorithm also with values 10,40 and 100 for $k$ and obtained the best results with $k=40$, where 83,48\% of the instances were correctly classified. Again the normalization of the input had no influence on the results. The evaluation of this algorithm took 120 seconds. In the following tables we present further results of the evaluation with $k=40$ including a confusion matrix.

\begin{table}[h]
\centering
\begin{tabular}{lllllll}
	\toprule
									&TP Rate   &FP Rate   &Precision   &Recall  &F-Measure &Class\\
	\midrule
									&0.593     &0.088      &0.68      &0.593     &0.633    &>50K\\
									&0.912     &0.407      &0.876     &0.912     &0.893    &$\leq$50K\\
  
	Weighted Avg.   &0.835     &0.331      &0.829     &0.835     &0.831     \\
	\bottomrule
\end{tabular}
\caption{Results of the kNN-classification for the adult dataset}
\end{table}

\vspace{6pt}

\begin{table}[h]
\centering
\begin{tabular}{|l|ll|}
	\hline
	 &              a    & b     \\
	\hline
    a = >50K      &	4647 & 3194  \\
    b = $\leq$50K & 2186 & 22534 \\
    \hline
\end{tabular}
\caption{Confusion matrix for kNN}
\end{table}


\subsection{Comparison}

The table below shos a comparison of the different algorithms on the adult dataset. For an algorithm we present here only the best obtained result concerning the TP rate. Values are always the weighted average.

\begin{table}[h]
\centering
\begin{tabular}{llllllll}
	\toprule
									&TP Rate   	&FP Rate   &Precision &Recall  &F-Measure   &  Correctly classified\\
	\midrule
	Decision Tree		& 0.86    	& 0.307   	& 0.854   & 0.86   & 0.855     & 		& FEHLT\\
  Random Forest		& 0.859   	& 0.333   	& 0.853   & 0.859  & 0.851     	& FEHLT\\
  Naive Bayes		 	&  0.83    & 0.186   & 0.854     & 0.83   & 0.837     \\
  SVM							& 0.849    	& 0.343     &0.842   	&0.849   &0.842     	& 0.849\\
  kNN							&0.835     	&0.331      &0.829    &0.835   &0.831				& 0.835\\
	\bottomrule
\end{tabular}
\caption{Comparison of the different algorithms for the adult dataset}
\end{table}



% ------------------------------------
% C O N T R A S E P T I V E    S E T
% ------------------------------------


\section{Contraceptive Method Choice Data}

\paragraph{}The Contraceptive Method Choice Data Set consists of information about the demographic and socio-economic characteristics of married women. There are 1473 instances with 9 different attributes. Examples of these attributes are the wife's age, husbands education, wife's religion, etc. The task is to predict which kind of contraceptive method a woman uses. These are categorized as no use, short term or long term. We performed a 10-fold-cross-validation on this dataset.

\subsection{Preprocessing}

\paragraph{}The values of the data sets's attributes are already in a numerical and categorical form, whereby the categories are encoded in an ordinal form. These go from 1 to 4 where 1 means low/bad and 4 means high/good. The number-of-children attribute is numeric but the values range from 0 to 16 so that a preprocessing is not necessary as one can think of 15 classes (15 distinct values). This set has no missing values and the attributes do not depend on each other. So we can use the data as is.


\subsection{Decision Tree}

With default settings, the J48 algorithm comes off very badly, in table \ref{tab:contra:dec:default} one can see a that it classifies about the half of all instances correctly. It must be apologetically supplemented that the task includes more distinct values, three instead of two nominals, to be predicted. 

\begin{table*}[htb]\centering
  \begin{tabular*}{\columnwidth}{@{}lllllll@{}}
      \toprule 
               &  TP Rate & FP Rate & Precision & Recall & F-Measure &  Class \\ \midrule 
               &  0.609   & 0.287   & 0.613     & 0.609  & 0.611     &  1     \\  
               &  0.378   & 0.138   & 0.445     & 0.378  & 0.409     &  2     \\      
               &  0.507   & 0.318   & 0.458     & 0.507  & 0.481     &  3     \\      
Weighted Avg.  &  0.521   & 0.264   & 0.521     & 0.521  & 0.52      &        \\ \bottomrule     
    \end{tabular*}
\caption{Decision Tree on Contraceptive Data Set -- Default Settings} 
\label{tab:contra:dec:default}
\end{table*}
\FloatBarrier

\paragraph{Raising the confidence factor}
As expected, raising the confidence factor worsens the result. For a series of a factor 0.25, 0.5, 0.25 and a totally unpruned tree, the quality decreases from originally 0.52\% to 50.4\%, 48.6\% and finally 48.7\%. The used decision tree grows from a size of 157 leaves and a maximum depth of 263 to a huge tree with 417 leaves and 655 branches. As said, this is a definitely wrong direction of optimizing.

\paragraph{Raising the minimum of instances per leaf}
We can notice a certain improvement to the outcome if we ensure that every leaf of the decision tree has to contain more instances, thereby preventing the tree from deep nesting. We can make the most out of it if we set the minimum to 23 leaves, the yield can be seen in table \ref{tab:contra:dec:23l}. This tree has only 31 leaves and 49 branches. Further enhancing of this parameter cuts the amount of correctly classified instances again.


\begin{table*}[htb]\centering
  \begin{tabular*}{\columnwidth}{@{}lllllll@{}}
      \toprule 
              &   TP Rate & FP Rate & Precision & Recall & F-Measure &  Class \\ \midrule      
              &   0.595   & 0.231   & 0.657     & 0.595  & 0.624     &  1     \\      
              &   0.405   & 0.135   & 0.467     & 0.405  & 0.434     &  2     \\      
              &   0.575   & 0.334   & 0.478     & 0.575  & 0.522     &  3     \\      
Weighted Avg. &   0.545   & 0.245   & 0.552     & 0.545  & 0.546     &        \\ \bottomrule     
    \end{tabular*}
\caption{Decision Tree on Contraceptive Data Set -- at least 23 instances per leaf} 
\label{tab:contra:dec:23l}
\end{table*}
\FloatBarrier
\paragraph{Other parameters}
Neither laplace correction nor mdl correction has any positive effect on the output of the algorithm on this data set. The absence of mdl correction make the outcome a bit worse, it brings the precision down to 0.551\%.  But there is one small tweak from which the accuracy of the algorithm can benefit: If binary splits are used on nominal attributes, we get the best result for the J48 on this data set as seen in table \ref{tab:contra:dec:bin}.

 
\begin{table*}[htb]\centering
  \begin{tabular*}{\columnwidth}{@{}lllllll@{}}
      \toprule 
               &  TP Rate & FP Rate & Precision & Recall & F-Measure &  Class \\ \midrule   
               &  0.649   & 0.263   & 0.648     & 0.649  & 0.648     &  1     \\    
               &  0.402   & 0.13    & 0.475     & 0.402  & 0.436     &  2     \\        
               &  0.548   & 0.292   & 0.499     & 0.548  & 0.522     &  3     \\    
Weighted Avg.  &  0.558   & 0.243   & 0.557     & 0.558  & 0.557     &        \\ \bottomrule 
    \end{tabular*}
\caption{Decision Tree on Contraceptive Data Set -- using binary splits} 
\label{tab:contra:dec:bin}
\end{table*}
\FloatBarrier

\subsection{Random Forest}
Random Forests perform even worse than the J48 with default settings. We can see in table \ref{tab:contra:rand:default} that only the half of all classified instances are classified correctly.

\begin{table*}[htb]\centering
  \begin{tabular*}{\columnwidth}{@{}lllllll@{}}
      \toprule 
               &  TP Rate & FP Rate & Precision & Recall & F-Measure  & Class  \\  \midrule      
               &  0.631   & 0.328   & 0.589     & 0.631  & 0.609      & 1      \\       
               &  0.36    & 0.16    & 0.397     & 0.36   & 0.378      & 2      \\       
               &  0.456   & 0.274   & 0.469     & 0.456  & 0.462      & 3      \\       
Weighted Avg.  &  0.509   & 0.271   & 0.504     & 0.509  & 0.506      &        \\  \bottomrule     
    \end{tabular*}
\caption{Random Forests on Contraceptive Data Set -- Default Settings} 
\label{tab:contra:rand:default}
\end{table*}
\FloatBarrier

\paragraph{More but shorter trees}
Random forests perform best on the data set with a maximum depth of 5 branches. After this improvement we can lift the precision up to 54.4\%. Furthermore, our random forest benefits from adding five more random trees so we can enhance the precision to  54.7\%.
\paragraph{More Features}
The most significant improvement can then observed after increasing the number of randomly chosen features. By default, the number of chosen features for this data set lays about 4 ( $ log_2(10) + 1 $ ). Choosing two more of them results in the best accuracy for random forests on this data set, as shown in table \ref{tab:contra:rand:6f}.

\begin{table*}[htb]\centering
  \begin{tabular*}{\columnwidth}{@{}lllllll@{}}
      \toprule 
               &  TP Rate & FP Rate & Precision & Recall & F-Measure &  Class \\  \midrule     
               &  0.604   & 0.231   & 0.661     & 0.604  & 0.631     &  1     \\  
               &  0.408   & 0.127   & 0.484     & 0.408  & 0.443     &  2     \\  
               &  0.593   & 0.326   & 0.491     & 0.593  & 0.537     &  3     \\  
Weighted Avg.  &  0.556   & 0.241   & 0.562     & 0.556  & 0.556     &        \\  \bottomrule 
    \end{tabular*}
\caption{Random Forests on Contraceptive Data Set -- with at least 6 chosen features} 
\label{tab:contra:rand:6f}
\end{table*}
\FloatBarrier


\subsection{Naive Bayes}

\paragraph{}The input data set does perfectly fit for Naive Bayes. But the classifier is not able to correctly predict the instances. As shown in tables \ref{tab:contra:bayes:1d} and \ref{tab:contra:bayes:1c} it gains a 50:50 chance of a TP prediction.


\begin{table*}[htb]\centering
  \begin{tabular*}{\columnwidth}{@{}lllllll@{}}
      \toprule 
               &  TP Rate & FP Rate & Precision & Recall & F-Measure & Class \\   \midrule
               &  0.52    & 0.223   & 0.635     & 0.52   & 0.572     & 1  \\  
               &  0.526   & 0.245   & 0.385     & 0.526  & 0.445     & 2  \\
               &  0.481   & 0.268   & 0.488     & 0.481  & 0.485     & 3  \\
Weighted Avg.  &  0.508   & 0.243   & 0.528     & 0.508  & 0.513     &    \\   \bottomrule
    \end{tabular*}
\caption{Naive Bayes} 
\label{tab:contra:bayes:1d}
\end{table*}



\begin{table}[h]
\centering
\begin{tabular}{|l|lll|}
\hline
              &   a &   b &   c  \\
\hline
 a = 1        & 327 & 141 & 161  \\
 b = 2        &  61 & 175 &  97  \\
 c = 3        & 127 & 138 & 246  \\
\hline
\end{tabular}
\caption{Confusion matrix for Naive Bayes}
\label{tab:contra:bayes:1c}
\end{table}


\subsection{Support Vector Machine}

\paragraph{}We tested different kernel functions on this dataset and obtained the best results with the NormalizedPolyKernel. The whole process of the evaluation, including the building of the models, took 28 seconds. Normalization of the data had again no influence on the results. The SVM algorithm was able to correctly classify 49,2\% of the instances. The following tables show further results of the evaluation with the NormalizedPolyKernel including a confusion matrix.

\begin{table}[htb]
\centering
\begin{tabular}{lllllll}
	\toprule
	&								TP Rate   	&FP Rate   &Precision   &Recall  &F-Measure     	&Class\\ 
	\midrule
									&0.583     	&0.355     &0.55      	&0.583   &0.566   		    	&1\\
                 	&0.393     	&0.181     &0.389     	&0.393   &0.391      	    	&2\\
                 	&0.444     	&0.252     &0.484     	&0.444   &0.463      	    	&3\\
	Weighted Avg.   & 0.849    & 0.343      &0.842   &  0.849     &0.842     			&\\
	\bottomrule
\end{tabular}
\caption{Results of the SVM-classification for the contraception dataset}
\end{table}

\vspace{6pt}

\begin{table}[h]
\centering
\begin{tabular}{|l|lll|}
	\hline
	      &a   &b   &c \\
	\hline
	a = 1 &367 &105 &157 \\
 	b = 2 &117 &131 &85 \\
 	c = 3 &183 &101 &227 \\
  \hline
\end{tabular}
\caption{Confusion matrix for SVM}
\end{table}


\subsection{k-Nearest Neighbour}

\paragraph{}We tested the kNN-algorithm with a lot of different values for $k$ and obtained the best results for $k=7$ where 48,88\% of the instances were correctly classified. The evaluation took not even a second. As with the other datasets the normalization of the input had no influence on the results. In the following tables we present further results of the evaluation with $k=7$ including a confusion matrix.

\begin{table}[h]
\centering
\begin{tabular}{lllllll}
	\toprule
	&								TP Rate   	&FP Rate   &Precision   &Recall  &F-Measure     	&Class\\
	\midrule
									&0.569     	&0.351     &0.547     	&0.569   &0.558      	     	&1\\
                 	&0.414     	&0.166     &0.422     	&0.414   &0.418      	    	&2\\
                 	&0.438     	&0.279     &0.455     	&0.438   &0.447      	    	&3\\

	Weighted Avg.   &0.489     	&0.284     &0.487     	&0.489   &0.488      				&\\
	\bottomrule
\end{tabular}
\caption{Results of the kNN-classification for the contraception dataset}
\end{table}

\vspace{6pt}

\begin{table}[h]
\centering
\begin{tabular}{|l|lll|}
	\hline
	      &a   &b   &c \\
	\hline
    a = 1 &358 &99  &172\\
    b = 2 &99  &138 &96 \\
 	c = 3 &197 &90  &224 \\
  \hline
\end{tabular}
\caption{Confusion matrix for kNN}
\end{table}

\subsection{Comparison}

The table below shos a comparison of the different algorithms on the adult dataset. For an algorithm we present here only the best obtained result concerning the TP rate. Values are always the weighted average.

\begin{table}[h]
\centering
\begin{tabular}{llllllll}
	\toprule
									&TP Rate   	&FP Rate   &Precision &Recall  &F-Measure   &  Correctly classified\\
	\midrule
	Decision Tree 	&  0.558   & 0.243   & 0.557     	& 0.558  & 0.557     & FEHLT\\
  Random Forest		&  0.556   & 0.241   & 0.562     	& 0.556  & 0.556     & FEHLT\\
  Naive Bayes		 	&  0.508   & 0.243   & 0.528     & 0.508  & 0.513 \\
  SVM							& 0.849    & 0.343   &0.842   		& 0.849  & 0.842 		 & 0.492\\
  kNN							&0.489     &0.284    &0.487     	& 0.489  & 0.488 		 & 0.489\\
	\bottomrule
\end{tabular}
\caption{Comparison of the different algorithms for the contraceptive method choice dataset}
\end{table}
\bibliography{references}


\end{document}
%------------------------------------------------


