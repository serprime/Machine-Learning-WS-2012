%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Short Sectioned Assignment
% LaTeX Template
% Version 1.0 (5/5/12)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Frits Wenneker (http://www.howtotex.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%   PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[paper=a4, fontsize=11pt]{scrartcl} % A4 paper and 11pt font size

% philipp: added for code listings
\usepackage{listings}

\usepackage{natbib}
\bibliographystyle{plain}

\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
\usepackage{fourier} % Use the Adobe Utopia font for the document - comment this line to return to the LaTeX default
\usepackage[english]{babel} % English language/hyphenation
\usepackage{amsmath,amsfonts,amsthm} % Math packages

\usepackage{lipsum} % Used for inserting dummy 'Lorem ipsum' text into the template
\usepackage{todonotes}

\usepackage{sectsty} % Allows customizing section commands
\allsectionsfont{\centering \normalfont\scshape} % Make all sections centered, the default font and small caps

\usepackage{fancyhdr} % Custom headers and footers
\pagestyle{fancyplain} % Makes all pages in the document conform to the custom headers and footers
\fancyhead{} % No page header - if you want one, create it in the same way as the footers below
\fancyfoot[L]{} % Empty left footer
\fancyfoot[C]{} % Empty center footer
\fancyfoot[R]{\thepage} % Page numbering for right footer
% \renewcommand{\thesection}{Data Set \arabic{section}}
\renewcommand{\headrulewidth}{0pt} % Remove header underlines
\renewcommand{\footrulewidth}{0pt} % Remove footer underlines
\setlength{\headheight}{13.6pt} % Customize the height of the header

\numberwithin{equation}{section} % Number equations within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{figure}{section} % Number figures within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{table}{section} % Number tables within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)

\setlength\parindent{0pt} % Removes all indentation from paragraphs - comment this line for an assignment with lots of text

%----------------------------------------------------------------------------------------
%   TITLE SECTION
%----------------------------------------------------------------------------------------

\newcommand{\horrule}[1]{\rule{\linewidth}{#1}} % Create horizontal rule command with 1 argument of height

\title{ 
\normalfont \normalsize 
\textsc{Vienna University of Technology} \\ [25pt] % Your university, school and/or department name(s)
\horrule{0.5pt} \\[0.4cm] % Thin top horizontal rule
\huge Experiments in Machine Learning \\ % The assignment title
\horrule{2pt} \\[0.5cm] % Thick bottom horizontal rule
}

\author{Benjamin Kiesl \and Philipp Steinwender \and Robert Sch\"{a}fer} % Your name

\date{\normalsize\today} % Today's date or a custom date

\begin{document}

\maketitle % Print the title

%----------------------------------------------------------------------------------------
%   PROBLEM 1
%----------------------------------------------------------------------------------------

\todo[inline]{Chose 5 different classifiers, from at least 3 different types
of learning algorithms

Argue \& justify choice (part of grading...)

Experiment with different parameter settings - And report on it !

Compare results among classifiers and datasets

Evaluate effect of pre-processing (e.g. different strategies
for missing values)
}

\section{Echocardiogram data}

The Echocardiogram data set consists of 132 entries with 13 attributes respectively and has 132 missing values in total. Originally, these entries referred to patients that suffered from a heart attack at some point in the past \cite{uci-repo}. The prediction task is to determine if the patient will survive at least one year.

\subsection{Preprocessing}
The name attribute of the patients was replaced by a dummy for all entries. For this reason, this attribute can be entirely removed. Another unnecessary attribute is ``group'' which is just meaningless and can be ignored \cite{uci-repo}. Another derived attribute is ``mult'' and can be ignored as well.

Some of the patients survived less than 12 months even if they were still alive at the end of the survey. In this case these patients joined the survey too recently to be followed at least one year. To make a reasonable prediction if a patient will survive at least one year, those entries have to be left out for the training data set because they would irritate any learning algorithm. \todo[inline]{maybe, give an example to show that those patients confuse a machine learning algorithm} 

Yet, there are some entries, which can be wiped out from the whole data set as well:

\missingfigure{Show the crappy entry with the 76 years old patient who has only null values}

So long, these preprocessing tasks are just removing attributes (column-wise) or removing certain entries (row-wise), depending on their values.

Unfortunately, there is another peculiarity with the actual prediction attribute ``alive-at-1'', here is the official description:
\begin{quotation} ``Boolean-valued. Derived from the first two attributes. 0 means patient was either dead after 1 year or had been followed for less than 1 year.  1 means patient was alive at 1 year.''\cite{uci-repo} \end{quotation}
De facto, the values don't fit to this description. The attribute is $1$ if the patient was followed less than one year and $0$ otherwise. Very astonishing: Even if the attribute is supposed to be a derived attribute, there are several \texttt{null} values, making the corresponding values useless for training data.                                                                

We thus inferred the prediction attribute ``alive-at-1'' as explained in the description of the whole dataset: For all patients where the months of survival are greater than or equal to 12 we set the ``alive-at-1''-attribute to 1, otherwise we set it to 0.

The only thing which remained then was to a

\todo{Compare results with/without preprocessing}

\subsection{Decision Tree}
%------------------------------------------------
On the first glimpse, it is an easy task for a decision based algorithm:

\begin{verbatim}
J48 pruned tree
------------------

survival <= 12
|   survival <= 11: 0 (4.0)
|   survival > 11: 1 (4.0)
survival > 12: 1 (87.0)
\end{verbatim}

That's correct, if the patient survived $ <= 12 $ months, he survived at least 12 month. What a statement! 

The only remarkable thing is, that the J48 implementation under default settings seems to enforce a certain depth of the tree, even if the splitting on the same attribute don't make any sense ($ 11 $ is yet a smaller number than $ 12 $, isn't it?). E.g. choosing a DecisionStump results in the expected single splitting: 
\begin{verbatim}
Class distributions
survival <= 11.5
0   1   
1.0 0.0 
survival > 11.5
0   1   
0.0 1.0 
survival is missing
0   1   
0.042105263157894736    0.9578947368421052
\end{verbatim}
Since there is no missing value for the survival attribute, this branch can be ignored.

But allowing the machine learning algorithms to see the origin attributes from which we derived the prediction attribut leads to absurdity, since the actual prediction task is to say whether a patient will survive one year after suffering a heart attack or not \emph{depending on the medical characteristics}.


\subsection{re-pre-processed the data set}

As mentioned in the description of the dataset on the UCI repo site we can remove the attributes group, name and mold as they do not have any meaning for the prediction attribute.

We also have to remove the survival and still-alive attributes, as the prediction attribute derives from them. Otherwise, a classification algorithm could predict all instances by taking into account only these two attributes.

A further refinement of the data set has been done by deleting all instances, where the outcome was unknown, as this cannot be used for training.




\section{Adult data}

The Adult or Census Income data sets contains 48842 instances of information about persons and if they earn more than 50.000 a year. The data set has 14 attributes and missing values.

The instances origin from the 1994 Census Database and were extracted by Barry Becker. For having a clean data set, only records with age above 16 with minimium one working hour per week have been selected from the database.

\subsection{Preprocessing}

We can remove the final weight -- but we have to find out what it means !!.

We can also remove the $education-num$ attribute as it is only a numeric representation of the education attribute.

At first we try to discretize all numeric attributes with 10 bins of equal ranges.




\subsection{Decision Tree}




\subsection{Random Forrest}




\subsection{Support Vector Machine}




\subsection{Naive Bayes}

We use 66\% of the data set for training and the rest for evaluation.

For the first run, we use the default setting and get the following result:

\begin{verbatim}
Correctly Classified Instances        9104               82.2329 %
Incorrectly Classified Instances      1967               17.7671 %
Total Number of Instances            11071     

Precision   Recall  F-Measure  Class
 0.592       0.786   0.675      >50K
 0.927       0.834   0.878      <=50K
\end{verbatim}

Second run: we use the discretizer with 20 bin.

\begin{verbatim}
Correctly Classified Instances        9287               83.8858 %
Incorrectly Classified Instances      1784               16.1142 %
Total Number of Instances            11071     

=== Detailed Accuracy By Class ===

Precision   Recall  F-Measure  Class
 0.621       0.805   0.701      >50K
 0.934       0.849   0.89       <=50K
\end{verbatim}

As expected, the result is slightly better.

A better result should be possible if we let the dicretizer split the values in part of nearly equal size. The result:

\begin{verbatim}
Correctly Classified Instances        9179               82.9103 %
Incorrectly Classified Instances      1892               17.0897 %
Total Number of Instances            11071     

Precision   Recall  F-Measure  Class
0.602        0.805   0.689      >50K
0.933        0.836   0.882      <=50K
\end{verbatim}

It got worse. \todo{why does discretizer with equal bin size not give a better result?}

The Naive Bayes does not have a lot of settings. It use a built in supervised discretizer or a kernel estimator. But the result is always the same with around 80\% correctly classified instances in the test run.

\subsection{k-Nearest}


\section{Contraceptive Method Choice Data}

The Contraceptive Method Choice Data Set consists of information about the demographic and socio-economic characteristics of married women. The task is to predict which kind of contraceptive methods a woman uses. These are categorized as no use, short term or long term.

\subsection{Preprocessing}

The attributes are already in a numerical and categorical form, whereby the categories are encoded in an ordinal form. These go from 1 to 4 where 1 is low/bad and 4 is high/good.

\todo{check if ordinal is the correct term when i mean, that the attributes are encoded in numbers, where 1<2, 1<4, 4>2, ...}


\subsection{Desicion Tree}
\subsection{Random Forrest}
\subsection{Naive Bayes}
\subsection{Support VectorMachine}
\subsection{k-Nearest}



\bibliography{references}


\end{document}
%------------------------------------------------


\subsection{Heading on level 2 (subsection)}

Lorem ipsum dolor sit amet, consectetuer adipiscing elit. 
\begin{align}
A = 
\begin{bmatrix}
A_{11} & A_{21} \\
A_{21} & A_{22}
\end{bmatrix}
\end{align}
Aenean commodo ligula eget dolor. Aenean massa. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Donec quam felis, ultricies nec, pellentesque eu, pretium quis, sem.

%------------------------------------------------

\subsubsection{Heading on level 3 (subsubsection)}

\lipsum[3] % Dummy text

\paragraph{Heading on level 4 (paragraph)}

\lipsum[6] % Dummy text

%----------------------------------------------------------------------------------------
%   PROBLEM 2
%----------------------------------------------------------------------------------------

\section{Lists}

%------------------------------------------------

\subsection{Example of list (3*itemize)}
\begin{itemize}
    \item First item in a list 
        \begin{itemize}
        \item First item in a list 
            \begin{itemize}
            \item First item in a list 
            \item Second item in a list 
            \end{itemize}
        \item Second item in a list 
        \end{itemize}
    \item Second item in a list 
\end{itemize}

%------------------------------------------------

\subsection{Example of list (enumerate)}
\begin{enumerate}
\item First item in a list 
\item Second item in a list 
\item Third item in a list
\end{enumerate}

%----------------------------------------------------------------------------------------



